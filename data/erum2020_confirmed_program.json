[
  {
    "title": "An enriched disease risk assessment model based on historical blood donors records",
    "author": "Andrea Cappozzo",
    "affiliation": "PhD student at University of Milan-Bicocca",
    "namesurname": "ANDREA,CAPPOZZO",
    "coauthor": "Edoardo Michielon, Alessandro De Bettin, Chiara D'Ignazio, Luigi Noto, Davide Drago, Alberto Prospero, Francesca De Chiara, Sergio Casartelli",
    "track": "R Applications",
    "session_type": "Lightning talk",
    "description": "Historically, the medical literature has largely focused on determining risk factors at an illness-specific level. Nevertheless, recent studies suggested that identical risk factors may cause the appearance of different diseases in different patients (Meijers & De Boer, 2019).\r\n\r\nThanks to the joint collaboration of Heartindata, a group of data scientists offering their passion and skills for social good, and Avis Milano, the Italian blood donor organization, an enriched disease risk assessment model is developed. Multiple risk factors and donations drop-out causes are collectively analyzed from AVIS longitudinal records, with the final aim of providing a broader and clearer overview of the interplay between risk factors and associated diseases in the blood donors population.",
    "email": "andrea.cappozzo@unimib.it"
  },
  {
    "title": "rdwd: R interface to German Weather Service data",
    "author": "Berry Boessenkool",
    "affiliation": "R trainer & consultant",
    "namesurname": "BERRY,BOESSENKOOL",
    "track": "R Applications",
    "session_type": "Lightning talk",
    "description": "rdwd is an R package to handle data from the German Weather Service (DWD). It allows to easily select, download and read observational data from over 6k weather stations. Both current data and historical records (partially dating back to the 1860s) are handled. Since about a year, gridded data from radar measurements can be read as well.",
    "email": "berryboessenkool@hotmail.com"
  },
  {
    "title": "tv: Show Data Frames in the Browser",
    "author": "Christoph Sax",
    "affiliation": "R-enthusiast, economist @cynkra",
    "namesurname": "CHRISTOPH,SAX",
    "coauthor": "Kirill Müller",
    "track": "R World",
    "session_type": "Lightning talk",
    "description": "The tv package lively displays data frames during data analysis.\r\nIt modifies the print method of data frames, tibbles or data tables to also appear in a browser or in the view pane of RStudio.\r\n\r\nThis is similar in spirit to the View() function in RStudio, works in other development environments, and has several advantages.\r\nChanges in data frame are shown immediately and next to the script and the console output, rather than on top of them.\r\nThe display keeps the position and the width of columns if a modified data frame is shown in tv.\r\nIt is updated asynchronously, without interrupting the analysis workflow.",
    "email": "christoph@cynkra.com"
  },
  {
    "title": "Predicting the Euro 2020 results using tournament rank probabilities scores from the socceR package",
    "author": "Claus Ekstrøm",
    "affiliation": "Statistician at University of Copenhagen. Longtime R hacker.",
    "namesurname": "CLAUS,EKSTRØM",
    "track": "R Applications",
    "session_type": "Lightning talk",
    "description": "The 2020 UEFA European Football Championship will be played this summer. Football championships are the source of almost endless predictions about the winner and the results of the individual matches, and we will show how the recently developed tournament rank probability score can be used to compare predictions.\r\n\r\nDifferent statistical models form the basis for predicting the result of individual matches. We present an R framework for comparing different prediction models and for comparing predictions about the Euro results. Everyone is encouraged to contribute their own function to make predictions for the result of the Euro 2020 championship.\r\n\r\nEach contributer will be shown how to provide two functions: a\r\nfunction that predicts the final score for a match between two teams with different skill levels, and a function that updates the\r\nskill levels based on the results of a given match. By supplying these two functions to the R framework the prediction results can be compared and the winner of the best football predictor can be found when Euro 2020 finishes.",
    "email": "ekstrom@sund.ku.dk"
  },
  {
    "title": "Differential Enriched Scan 2 (DEScan2): an R pipeline for epigenomic analysis.",
    "author": "Dario Righelli",
    "affiliation": "Department of Statistics, University of Padua, Post-Doc",
    "namesurname": "DARIO,RIGHELLI",
    "coauthor": "Koberstein John, Gomes Bruce, Zhang Nancy, Angelini Claudia, Peixoto Lucia, Risso Davide",
    "track": "R Life Sciences",
    "session_type": "Lightning talk",
    "description": "We present DEScan2, a R/Bioconductor package for the differential enrichment analysis of epigenomic sequencing data.\r\nOur method consists of three steps: peak caller, peak consensus across samples, and peak signal quantification.\r\nThe peak caller is a standard moving scan window comparing the counts between a sliding window and a larger region outside the window, using a Poisson likelihood, providing a z-score for each peak. However, the package can work with any external peak caller: to this end, we provide additional functionalities to load peaks from bed files and handle them as internal optimized structures.\r\nThe consensus step aims to determine if a peak is a \"true peak\" based on its replicability across samples: we developed a filtering step to filter out those peaks not present in at least a user given number of samples. A further threshold can be used over the peak z-scores.\r\nFinally, the third step produces a count matrix where each column is a sample and each row a previously filtered peak. The value of each matrix cell is the number of reads for the peak in the sample.\r\nFurthermore, our package provides several functionalities for common genomic data structure handling, for instance, to give the possibility to split the data over the chromosomes to speed-up the computations parallelizing them on multiple CPUs.",
    "email": "dario.righelli@unipd.it"
  },
  {
    "title": "Ultra fast penalized regressions with R package {bigstatsr}",
    "author": "Florian Privé",
    "affiliation": "Postdoc at Aarhus University",
    "namesurname": "FLORIAN,PRIVÉ",
    "track": "R Machine Learning & Models",
    "session_type": "Lightning talk",
    "description": "In this talk, I introduce the implementations of penalized linear and logistic regressions as implemented in R package {bigstatsr}.\r\nThese implementations use data stored on disk to handle very large matrices.\r\nThey automatically perform a procedure similar to cross-validation to choose the two hyper-parameters, λ and α, of the elastic net regularization, in parallel.\r\nThey employ an early stopping criterion to avoid fitting very expensive models, making these implementations on average 10 times faster than with {glmnet}.\r\nHowever, package {bigstatsr} does not implement all the many models and options provided by the excellent package {glmnet}; some are area of future development.",
    "email": "florian.prive.21@gmail.com"
  },
  {
    "title": "Supporting Twitter analytics application with graph-databases and the aRangodb package",
    "author": "Gabriele Galatolo",
    "affiliation": "Kode Srl, Software Developer & Data Scientist",
    "namesurname": "GABRIELE,GALATOLO",
    "coauthor": "Francesca Giorgolo, Ilaria Ceppa, Marco Calderisi, Davide Massidda, Matteo Papi, Andrea Spinelli, Andrea Zedda, Jacopo Baldacci, Caterina Giacomelli",
    "track": "R Applications",
    "session_type": "Lightning talk",
    "description": "The importance of finding efficient ways to model and to store unstructured data has incredibly grown in the last decade, in particular with the strong expansion of social-media services. Among those storing tools an increasingly important class of databases is represented by the graph-oriented databases, where relationships between data are considered first-class citizens.\r\nIn order to support the analyst or the data scientist to interact and use in a simple way with this paradigm, we developed last year the package aRangodb, an interface with the graph-oriented database ArangoDB.\r\nTo show the capabilities of the package and of the underlying way to model data using graphs we present Tweetmood, a tool to analyze and visualize tweets from Twitter.\r\nIn this talk, we will present some of the most significant features of the package applied in the Tweetmood context, such as functionalities to traverse the graph and some examples in which the user can elaborate those graphs to get new information that can easily be stored using the functions and the tools available in the package.",
    "email": "g.galatolo@kode-solutions.net"
  },
  {
    "title": "Reproducible Data Visualization with CanvasXpress",
    "author": "Ger Inberg",
    "affiliation": "Freelance Analytics Developer",
    "namesurname": "GER,INBERG",
    "track": "R Dataviz & Shiny",
    "session_type": "Lightning talk",
    "description": "canvasXpress was developed as the core visualization component for bioinformatics and systems biology analysis at Bristol-Myers Squibb. It supports a large number of visualizations to display scientific and non-scientific data. canvasXpress also includes a simple and unobtrusive user interface to explore complex data sets, a sophisticated and unique mechanism to keep track of all user customization for Reproducible Research purposes, as well as an 'out of the box' broadcasting capability to synchronize selected data points in all canvasXpress plots in a page. Data can be easily sorted, grouped, transposed, transformed or clustered dynamically. The fully customizable mouse events as well as the zooming, panning and drag-and-drop capabilities are features that make this library unique in its class.",
    "email": "ginberg@gmail.com"
  },
  {
    "title": "Design your own quantum simulator with R",
    "author": "Indranil Ghosh",
    "affiliation": "Final year post Graduate student from the department of Physics, Jadavpur University, Kolkata, India",
    "namesurname": "INDRANIL,GHOSH",
    "track": "R Applications",
    "session_type": "Lightning talk",
    "description": "The main idea of the project is to use the R ecosystem to write computer codes for designing a quantum simulator, for simulating different quantum algorithms. I will start with giving a brief introduction to linear algebra for starting with quantum computation, and how to write your own R codes from scratch to implement them. Then I will take a dive into implementing simple quantum circuits starting with initializing qubits and terminating with a measurement. I will also implement simple quantum algorithms concluding with giving a brief intro to quantum game theory and their simulations with R.",
    "email": "indranilg49@gmail.com"
  },
  {
    "title": "What are the potato eaters eating",
    "author": "Keshav Bhatt",
    "affiliation": "R-fan and independent researcher",
    "namesurname": "KESHAV,BHATT",
    "track": "R Applications, R Dataviz & Shiny",
    "session_type": "Lightning talk",
    "description": "Although stereotypes can quite useful they are often not correct. For instance, the Dutch are stereotyped as being potato eaters. While this might have been historically correct, it is not currently accurate. The Dutch sparingly eat potatoes and this paper uses data to disprove the stereotype. To get an impression of Dutch food habits, a popular local website was scraped. Besides its popularity, the website hosts user-generated content, giving a good proxy of Dutch taste-buds.\r\nWhile it was apparent on the website, lasagna is the most popular dish. Detailed NLP analysis of more than 50,000 recipes showed that potato based dishes are in fact nowhere at the top. This vindicated my belief. Moreover, it shows that the Dutch kitchen is globalizing. Tomato, a hallmark of South Europe is more popular than the Dutch potato. Also observed is the popularity of many herbs in the recipes, which are not a traditional component of the Dutch kitchen. \r\nThe world is changing and our kitchens too.\r\nThis trend will also be explored for other countries also.",
    "email": "bhatt.keshav@gmail.com"
  },
  {
    "title": "dm: working with relational data models in R",
    "author": "Kirill Müller",
    "affiliation": "Clean code, tidy data. Consulting for cynkra, coding in the open.",
    "namesurname": "KIRILL,MÜLLER",
    "track": "R Applications, R Production, R World",
    "session_type": "Lightning talk",
    "description": "Storing all data related to a problem in a single table or data frame (\"the dataset\") can result in many repetitive values. Separation into multiple tables helps data quality but requires \"merge\" or \"join\" operations. {dm} is a new package that fills a gap in the R ecosystem: it makes working with multiple tables just as easy as working with a single table.\r\n\r\nA \"data model\" consists of tables (both the definition and the data), and primary and foreign keys. The {dm} package combines these concepts with data manipulation powered by the tidyverse: entire data models are handled in a single entity, a \"dm\" object.\r\n\r\nThree principal use cases for {dm} can be identified:\r\n\r\n1. When you consume a data model, {dm} helps access and manipulate a dataset consisting of multiple tables (database or local data frames) through a consistent interface.\r\n\r\n2. When you use a third-party dataset, {dm} helps normalizing the data to remove redundancies as part of the cleaning process.\r\n\r\n3. To create a relational data model, you can prepare the data using R and familiar tools and seamlessly export to a database.\r\n\r\nThe presentation revolves around these use cases and shows a few applications. The {dm} package is available on GitHub and will be submitted to CRAN in early February.",
    "email": "kirill@cynkra.com"
  },
  {
    "title": "Explaining black-box models with xspliner to make deliberate business decisions",
    "author": "Krystian Igras",
    "affiliation": "Data Scientists and Software Engineer at Appsilon",
    "namesurname": "KRYSTIAN,IGRAS",
    "track": "R Machine Learning & Models",
    "session_type": "Lightning talk",
    "description": "A vast majority of the state of the art ML algorithms are black boxes, meaning it is difficult to understand their inner workings. The more that algorithms are used as decision support systems in everyday life, the greater the necessity of understanding the underlying decision rules. This is important for many reasons, including regulatory issues as well as making sure that the model learned sensible features. You can achieve all that with the xspliner R package that I have created.\r\n\r\nOne of the most promising methods to explain models is building surrogate models. This can be achieved by inferring Partial Dependence Plot (PDP) curves from the black box model and building Generalised Linear Models based on these curves. The advantage of this approach is that it is model agnostic, which means you can use it regardless of what methods you used to create your model.\r\n\r\nFrom this presentation, you will learn what PDP curves and GLMs are and how you can calculate them based on black box models. We will take a look at an interesting business use case in which we'll find out whether the original black box model or the surrogate one is a better decision system for our needs. Finally, we will see an example of how you can explain your models using this approach with the xspliner package for R (already available on CRAN!).",
    "email": "krystian@appsilon.com"
  },
  {
    "title": "Using open-access data to derive genome composition of emerging viruses",
    "author": "Liam Brierley",
    "affiliation": "MRC Skills Development Fellow, University of Liverpool",
    "namesurname": "LIAM,BRIERLEY",
    "coauthor": "Anna Auer-Fowler, Maya Wardeh, Matthew Baylis, Prudence Wong",
    "track": "R Life Sciences",
    "session_type": "Lightning talk",
    "description": "Outbreaks of new viruses continue to threaten global health, including pandemic influenza, Ebola virus, and the novel coronavirus ‘nCoV-2019’. Advances in genome sequencing allow access to virus RNA sequences on an unprecedented scale, representing a powerful tool for epidemiologists to understand new viral outbreaks. \r\n\r\nWe use NCBI’s GenBank, a curated open-access repository containing >200 million genetic sequences (3 million viral sequences) directly submitted by users, representing many individual studies. However, the resulting breadth of data and inconsistencies in metadata present consistent challenges.\r\n\r\nWe demonstrate our approach using R to address these challenges and a need for reproducibility as data increases. Firstly, we use `rentrez` to programmatically search, filter, and obtain virus sequences from GenBank. Secondly, we use `taxize` to resolve pervasive problems of naming conflicts, as virus names are often recorded differently between entries, partly because virus classification is complex and regularly revised. We successfully resolve 428 mammal and bird RNA viruses to species level before extracting sequences.\r\n\r\nObtaining genome sequences of a large inventory of viruses allows us to estimate genomic composition biases, which show promise in predicting virus epidemiology. Ultimately, this pathway will allow better quantification of future epidemic threats.",
    "email": "liam.brierley@liverpool.ac.uk"
  },
  {
    "title": "A principal component analysis based method to detect biomarker captation from vibrational spectra",
    "author": "Marco Calderisi",
    "affiliation": "Kode srl, CTO",
    "namesurname": "MARCO,CALDERISI",
    "coauthor": "Francesca Giorgolo, Ilaria Ceppa, Davide Massidda, Matteo Papi, Gabriele Galatolo, Andre Spinelli, Andrea Zedda, Jacopo Baldacci, Caterina Giacomelli, marco cecchini, matteo agostini",
    "track": "R Life Sciences",
    "session_type": "Lightning talk",
    "description": "BRAIKER is a microfluidics-Based biosensor aimed to detect biomarkers. The device is responsive to changes of mass and viscosity over its surface. When selected markers react with the sensor, a variation of resonant acoustic frequencies (called harmonics) is produced. A serious problem when examining the data produced by biosensors is the subjectivity of standard method to evaluate the pattern of harmonics. In our research, a method based on the principal component analysis has been applied on vibrational data. An R-Shiny application was developed in order to present data visualizations and multivariate analyses of vibrational spectra. The Shiny application allows to clean and explore data by using interactive data visualisation tools. The principal component analysis is applied to analyse simultaneously the full set of frequencies for multiple experimental runs, reducing the multivariate data set into a small number of components accounting for a component of variance near to that the original data. Functionalised and non-functionalised resonating foils of biosensor can be classified in order to validate the capability of the device to detect biomarkers, lowering the LOD and increasing sensitivity and resolution.",
    "email": "m.calderisi@kode-solutions.net"
  },
  {
    "title": "An innovative way to support your sales force",
    "author": "Matilde Grecchi",
    "affiliation": "Head of Data Science & Innovation @ZucchettiSpa",
    "namesurname": "MATILDE,GRECCHI",
    "track": "R Production, R Dataviz & Shiny, R Machine Learning & Models, R Applications",
    "session_type": "Lightning talk",
    "description": "Explanation of the web application realized in Shiny and deployed in production to support the sales force of Zucchetti. An overview of the overall step followed from data ingestion to modeling, from validation of the model to shiny web-app realization, from deployment in production to continous learning thanks to feedbacks coming from sales force and redemption of customers. All the code is written in R using RStudio. The deployment of the app is done with ShinyProxy.io",
    "email": "ing.maty@gmail.com"
  },
  {
    "title": "ptmixed: an R package for flexible modelling of longitudinal overdispersed count data",
    "author": "Mirko Signorelli",
    "affiliation": "Dept. of Biomedical Data Sciences, Leiden University Medical Center",
    "namesurname": "MIRKO,SIGNORELLI",
    "coauthor": "Roula Tsonaka, Pietro Spitali",
    "track": "R Machine Learning & Models, R Life Sciences",
    "session_type": "Lightning talk",
    "description": "Overdispersion is a commonly encountered feature of count data, and it is usually modelled using the negative binomial (NB) distribution. However, not all overdispersed distributions are created equal: while some are severely zero-inflated, other exhibit heavy tails.\r\nMounting evidence from many research fields suggests that often NB models cannot fit sufficiently well heavy-tailed or zero-inflated counts. It has been proposed to solve this problem by using the more flexible Poisson-Tweedie (PT) family of distributions, of which the NB is special case. However, current methods based on the PT can only handle cross-sectional datasets and no extension for correlated data is available.\r\nTo overcome this limitation we propose a PT mixed-effects model that can be used to flexibly model longitudinal overdispersed counts. To estimate this model we develop a computational pipeline that uses adaptive quadratures to accurately approximate the likelihood of the model, and numeric optimization methods to maximize it. We have implemented this approach in the R package ptmixed, which is published on CRAN.\r\nBesides showcasing the package’s functionalities, we will present an assessment of the accuracy of our estimation procedure, and provide an example application where we analyse longitudinal RNA-seq data, which often exhibit high levels of zero-inflation and heavy tails. \r\n\r\nReference: Signorelli, M., Spitali, P., Tsonaka, R. (2020, in press). Poisson-Tweedie mixed-effects model: a flexible approach for the analysis of longitudinal RNA-seq data. To appear in *Statistical Modelling*. arXiv preprint: [arXiv:2004.11193](https://arxiv.org/abs/2004.11193)",
    "email": "m.signorelli@lumc.nl"
  },
  {
    "title": "One-way non-normal ANOVA in reliability analysis using with doex",
    "author": "Mustafa CAVUS",
    "affiliation": "PhD Student @Eskisehir Technical University",
    "namesurname": "MUSTAFA,CAVUS",
    "coauthor": "Berna YAZICI",
    "track": "R Production, R Life Sciences, R Applications",
    "session_type": "Lightning talk",
    "description": "One-way ANOVA is used for testing equality of several population means in statistics, and current packages in R provides functions to apply it. However, the violation of its assumptions are normality and variance heterogeneity limits its use, also not possible in some cases. doex provides alternative statistical methods to solve this problem. It has several tests based on generalized p-value, parametric bootstrap and fiducial approaches for the violation of variance heterogeneity and normality. Moreover, it provides the newly proposed methods for testing equality of mean lifetimes under different failure rates. \r\n\r\nThis talk introduces doex package provides has several methods for testing equality of population means independently the strict assumptions of ANOVA. An illustrative example is given for testing equality of mean of product lifetimes under different failure rates.",
    "email": "mustafacavus@eskisehir.edu.tr"
  },
  {
    "title": "Keeping on top of R in Real-Time, High-Stakes trading systems",
    "author": "Nicholas Jhirad",
    "affiliation": "Senior Data Scientist, CINQ ICT (on Contract to Pinnacle Sports)",
    "namesurname": "NICHOLAS,JHIRAD",
    "coauthor": "Aaron Jacobs",
    "track": "R Production",
    "session_type": "Lightning talk",
    "description": "Visibility is the key to production. For R to work inside that environment, we need ubiquitous logging. I'll share insights from our experience building a production-grade R stack and monitoring all of our R applications via syslog, the 'rsyslog' package (on CRAN) and splunk.",
    "email": "shapenaji@gmail.com"
  },
  {
    "title": "Towards more structured data quality assessment in the process mining field: the DaQAPO package",
    "author": "Niels Martin",
    "affiliation": "Postdoctoral researcher Research Foundation Flanders (FWO) - Hasselt University",
    "namesurname": "NIELS,MARTIN",
    "coauthor": "Niels Martin (Research Foundation Flanders FWO - Hasselt University), Greg Van Houdt (Hasselt University), Gert Janssenswillen (Hasselt University)",
    "track": "R Applications",
    "session_type": "Lightning talk",
    "description": "Process mining is a research field focusing on the extraction of insights on business processes from process execution data embedded in files called event logs. Event logs are a specific data structure originating from information systems supporting a business process such as an Enterprise Resource Planning System or a Hospital Information System. As a research field, process mining predominantly focused on the development of algorithms to retrieve process insights from an event log. However, consistent with the “garbage in - garbage out”-principle, the reliability of the algorithm’s outcomes strongly depends upon the data quality of the event log. It has been widely recognized that real-life event logs typically suffer from a multitude of data quality issues, stressing the need for thorough data quality assessment. Currently, event log quality is often judged on an ad-hoc basis, entailing the risk that important issues are overlooked. Hence, the need for a more structured data quality assessment approach within the process mining field. Therefore, the DaQAPO package has been developed, which is an acronym for Data Quality Assessment of Process-Oriented data. It offers an extensive set of functions to automatically identify common data quality problems in process execution data. In this way, it is the first R-package which supports systematic data quality assessment for event data.",
    "email": "niels.martin@uhasselt.be"
  },
  {
    "title": "Analyzing Preference Data with the BayesMallows Package",
    "author": "Øystein Sørensen",
    "affiliation": "Associate Professor, University of Oslo",
    "namesurname": "ØYSTEIN,SØRENSEN",
    "coauthor": "Marta Crispino, Qinghua Liu, Valeria Vitelli",
    "track": "R Machine Learning & Models",
    "session_type": "Lightning talk",
    "description": "BayesMallows is an R package for analyzing preference data in the form of rankings with the Mallows rank model, and its finite mixture extension, in a Bayesian framework. The model is grounded on the idea that the probability density of an observed ranking decreases exponentially with the distance to the location parameter. It is the first Bayesian implementation that allows wide choices of distances, and it works well with a large number of items to be ranked. BayesMallows handles non-standard data: partial rankings and pairwise comparisons, even in cases including non-transitive preference patterns. The Bayesian paradigm allows coherent quantification of posterior uncertainties of estimates of any quantity of interest. These posteriors are fully available to the user, and the package comes with convenient tools for summarizing and visualizing the posterior distributions.\r\n\r\nThis talk will focus on how the BayesMallows package can be used to analyze preference data, in particular how the Bayesian paradigm allows endless possibilities in answering questions of interest with the help of visualization of posterior distributions. Such posterior summaries can easily be communicated with scientific collaborators and business stakeholders who may not be machine learning experts themselves.",
    "email": "oystein.sorensen.1985@gmail.com"
  },
  {
    "title": "Predicting Business Cycle Fluctuations Using Text Analytics",
    "author": "Sami Diaf",
    "affiliation": "Researcher at the University of Hamburg",
    "namesurname": "SAMI,DIAF",
    "track": "R Machine Learning & Models",
    "session_type": "Lightning talk",
    "description": "The use of computational linguistics proved to be crucial in studying macroeconomic forecasts and understanding the essence of such exercises. \r\n\r\nCombining machine learning algorithms with text mining pipelines helps dissecting potential patterns of forecast errors and investigates the role of ideology in such outcomes.\r\n\r\nThe Priority Program “Exploring the Experience-Expectation Nexus” builds up, from a large database of German business cycle reports, advanced topic models and predictive analytics to investigate the role of ideology in the production of macroeconomic forecasts. The pipelines call for advanced data processing, predicting business fluctuations from text covariates, measuring ideological stances of forecasters and explaining what influences forecast errors.",
    "email": "sami.diaf@uni-hamburg.de"
  },
  {
    "title": "Flexible deep learning via the JuliaConnectoR",
    "author": "Stefan Lenz",
    "affiliation": "Statistician at the Institute of Medical Biometry and Statistics (IMBI), Faculty of Medicine and Medical Center – University of Freiburg",
    "namesurname": "STEFAN,LENZ",
    "coauthor": "Harald Binder",
    "track": "R Machine Learning & Models",
    "session_type": "Lightning talk",
    "description": "For deep learning in R, frameworks from other languages, e. g. from Python, are widely used. Julia is another language which offers computational speed and a growing ecosystem for machine learning, e. g. with the package “Flux”. Integrating functionality of Julia in R is especially promising due to the many commonalities of Julia and R. We take advantage of these in the design of our “JuliaConnectoR” R package, which aims at a tight integration of Julia in R. We would like to present our package, together with some deep learning examples.\r\nThe JuliaConnectoR can import Julia functions, also from whole packages, and make them directly callable in R. Values and data structures are translated between the two languages. This includes the management of objects holding external resources such as memory pointers. The possibility to pass R functions as arguments to Julia functions makes the JuliaConnectoR a truly functional interface. Such callback functions can, e. g., be used to interactively display the learning process of a neural network in R while it is trained in Julia. Among others, this feature sets the JuliaConnectoR apart from the other R packages for integrating Julia in R, “XRJulia” and “JuliaCall”. This becomes possible with an optimized communication protocol, which also allows a highly efficient data transfer, leveraging the similarities in the binary representation of values in Julia and R.",
    "email": "lenz@imbi.uni-freiburg.de"
  },
  {
    "title": "Time Series Missing Data Visualizations",
    "author": "Steffen Moritz",
    "affiliation": "Institute for Data Science, Engineering, and Analytics, TH Köln",
    "namesurname": "STEFFEN,MORITZ",
    "coauthor": "Thomas Bartz-Beielstein",
    "track": "R Dataviz & Shiny, R Applications",
    "session_type": "Lightning talk",
    "description": "Missing data is a quite common problem for time series, which usually also complicates later analysis steps.\r\nIn order to deal with this problem, visualizing the missing data is a very good start. \r\n\r\nVisualizing the patterns in the missing data can provide more information about the reasons for the missing data and give hints on how to best proceed with the analysis.\r\n\r\nThis talk gives a short intro into the new plotting functions being introduced with the 3.1 version of the imputeTS CRAN package.",
    "email": "steffen.moritz10@gmail.com"
  },
  {
    "title": "effectclass: an R package to interpret effects and visualise uncertainty",
    "author": "Thierry Onkelinx",
    "affiliation": "Statistician at the Research Institute for Nature and Forest",
    "namesurname": "THIERRY,ONKELINX",
    "track": "R Dataviz & Shiny",
    "session_type": "Lightning talk",
    "description": "The package classifies effects by comparing their confidence interval with a reference, a lower and an upper threshold, all of which are set by the user a priori. The null hypothesis is a good choice as reference. The lower and upper threshold define a region around the reference in which the effect is small enough to be irrelevant. These thresholds are ideally based on the effect size used in the statistical power analysis of the design. Otherwise they can be based on expert judgement.\r\n\r\nThe result is a ten-scale classification of the effect. Three classes exist for significant effects above the reference and three classes for significant effects below the reference. The remaining four classes split the non-significant effects. The most important distinction is between “no effect” and “unknown effect”.\r\n\r\neffectclass provides ggplot2 add-ons stat_effect() and scale_effect() to visualise the effects as points with shapes depending on the classification. It provides stat_fan() which displays the uncertainty as multiple overlapping intervals with different confidence probability. stat_fan() is inspired by Britton, E.; Fisher, P. & J. Whitley (1998)\r\n\r\nMore details on the package website: https://effectclass.netlify.com/\r\n\r\nBritton, E.; Fisher, P. & J. Whitley (1998). The Inflation Report Projections: Understanding the Fan Chart. Bank of England Quarterly Bulletin.",
    "email": "thierry.onkelinx@inbo.be"
  },
  {
    "title": "A flexible dashboard for monitoring platform trials",
    "author": "Alessio Crippa",
    "affiliation": "Karolinska Institutet, postdoc",
    "namesurname": "ALESSIO,CRIPPA",
    "coauthor": "Andrea Discacciati, Erin Gabriel, Martin Eklund",
    "track": "R Applications",
    "session_type": "Poster",
    "description": "The Data and Safety Monitoring Board (DSMB) is an essential component for a successful clinical trial. It consists of an independent group of experts that periodically revise and evaluate the accumulating data from an ongoing trial to assess patients’ safety, study progress, and drug efficacy. Based on their evaluation, a recommendation to continue, modify or stop the trial will be delivered to the trial’s sponsor. It is essential to provide the DSMB with the best delivery visualization tools for monitoring on a regular basis the live data from the study trial.\r\nWe designed and developed an interactive dashboard using flexdashboard for R as a helping tool for assisting the DSMB in the evaluation of the results of the ProBio study, a clinical platform for improving treatment decision in patients with metastatic castrate resistant prostate cancer. We will focus on the customized structure for best displaying the most interesting variables and the adoption of interactive tools as a particularly useful aid for the assessment of the ongoing data. We will also cover the connection to the data sources, the automatic generation process, and the selected permission for the people in the DSMB to access the dashboard.",
    "email": "alessio.crippa@ki.se"
  },
  {
    "title": "PRDA package: Enhancing Statistical Inference via Prospective and Retrospective Design Analysis.",
    "author": "Angela Andreella",
    "affiliation": "University of Padua",
    "namesurname": "ANGELA,ANDREELLA",
    "coauthor": "Vesely Anna, Zandonella Callegher Claudio, Pastore Massimiliano, Altoè Gianmarco",
    "track": "R Life Sciences",
    "session_type": "Poster",
    "description": "There is a growing recognition of the importance of power analysis and calculation of the appropriate sample size when planning a research experiment. However, power analysis is not the only relevant aspect of the design of an experiment. Other inferential risks, such as the probability of estimating the effect in the wrong direction or the average overestimation of the actual effects, are also important. The evaluation of these inferential risks as well as the statistical power, in what Gelman and Carlin (2014) defined as Design Analysis, may help researchers to make informed choices both when planning an experiment or evaluating study results.\r\nWe introduce the PRDA (Prospective and Retrospective Design Analysis) package that allows researchers to carry a Design Analysis under different experimental scenarios (Altoè et al., 2020). Considering a plausible effect size (or its prior distribution) researchers can evaluate either the inferential risks for given sample size or the required sampled size to obtain a given statistical power.\r\nPreviously, PRDA functions were limited to mean differences between groups considering Cohen’s d in the Null significance Hypothesis Testing (NHST) framework. Now, we present the newly developed features that include other effect sizes (such as Pearson’s correlation) as well as Bayes Factor hypothesis testing.",
    "email": "angela.andreella@studenti.unipd.it"
  },
  {
    "title": "Automate flexdashboard with GitHub",
    "author": "Binod Jung Bogati",
    "affiliation": "Data Analyst Intern at VIN",
    "namesurname": "BINOD,JUNG,BOGATI",
    "track": "R Dataviz & Shiny",
    "session_type": "Poster",
    "description": "flexdashboard is a great tool for building an interactive dashboard in R. We can host it for free on GitHub Pages, Rpubs and many other places.\r\n\r\nHosted flexdashboard is static so changes in our data we need to manually update and publish every time. If we want to auto-update we may need to integrate Shiny. However, it may not be suitable for every case.\r\n\r\nTo overcome this, we have a solution called GitHub Action. It's a feature from GitHub which automates our tasks in a convenient way. \r\n\r\nWith the help of GitHub Actions, we can automate our flexdashboard (Rmarkdown) updates. It builds a container that runs our R scripts. We can trigger it every time we push on GitHub or schedule it every X minutes/hours/days/month. \r\n\r\nIf you want to learn more about the GitHub Action. And also know how to automate updates on your flexdashboard. Please do come and join me.",
    "email": "bjungbogati@gmail.com"
  },
  {
    "title": "EasyReporting: a Bioconductor package for Reproducible Research implementation",
    "author": "Dario Righelli",
    "affiliation": "Department of Statistics, University of Padua, Post-Doc",
    "namesurname": "DARIO,RIGHELLI",
    "coauthor": "angelini claudia",
    "track": "R Applications, R Life Sciences",
    "session_type": "Poster",
    "description": "EasyReporting is a novel R/Bioconductor package for speeding up Reproducible Research (RR) implementation when analyzing data, implementing workflows or other packages.\r\nIt is an S4 class helping developers to integrate an RR layer inside their software products, as well as data analysts speeding up their report production without learning the rmarkdown language.\r\nThanks to minimal additional efforts by developers, the end-user has available a rmarkdown file within all the source code generated during the analysis, divided into Code Chunks (CC) ready for the compilation. Moreover, EasyReporting gives also the possibility to add natural language comments and textual descriptions into the final report to be compiled for producing an enriched document that incorporates input data, source code and output results. Once compiled, the final document can be attached to the publication of the analysis as supplementary material, helping the interested community to entirely reproduce the computational part of work.\r\nDespite other previously proposed solutions, that usually require a significant effort by the final user, potentially bringing him/her to renounce to include RR inside the scripts, our approach is versatile and easy to be incorporated, allowing to the final developer/analyzer to automatically create and store a rmarkdown document, and providing also methods for its compilation.",
    "email": "dario.righelli@unipd.it"
  },
  {
    "title": "NewWave: a scalable R package for the dimensionality reduction of single-cell RNA-seq",
    "author": "Federico Agostinis",
    "affiliation": "Università degli studi di Padova, Fellowship",
    "namesurname": "FEDERICO,AGOSTINIS",
    "coauthor": "Chiara Romualdi, Gabriele Sales, Davide Risso",
    "track": "R Life Sciences",
    "session_type": "Poster",
    "description": "The fast development of single cell sequencing technologies in the recent\r\nyears has generated a gap between the throughput of the experiments and the\r\ncapability of analizing the generated data.\r\nOne recent method for dimensionality reduction of single-cell RNA-seq data is\r\nzinbwave, it uses zero inflated negative binomial likelihood function optimization\r\nto find biological meaningful latent factors and remove batch effect. Zinbwave\r\nhas optimal performance but has some scalability issues due to large memory\r\nusage. To address this, we developed an R package with new software architec-\r\nture extending zinbwave.\r\nIn this package, we implement mini-batch stochastic gradient descent and the\r\npossibility of working with HDF5 files. We decide to use a negative binomial\r\nmodel following the observation that droplet sequencing technologies do not\r\ninduce zero inflation in the data. Thanks to these improvements and the possi-\r\nbility of massively parallelize the estimation process using PSOCK clusters, we\r\nare able to speed up the computations with the same or even better results than\r\nzinbwave. This type of parallelization can be used on multiple hardware setups,\r\nranging from simple laptops to dedicated server clusters. This, paired with the\r\nability to work with out-of-memory data, enables us to analyze datasets with\r\nmilions of cells.",
    "email": "federico.agostinis@gmail.com"
  },
  {
    "title": "orf: Ordered Random Forests",
    "author": "Gabriel Okasa",
    "affiliation": "Research Assistant and PhD Candidate at the Swiss Institute for Empirical Economic Research, University of St. Gallen, Switzerland",
    "namesurname": "GABRIEL,OKASA",
    "coauthor": "Michael Lechner",
    "track": "R Machine Learning & Models",
    "session_type": "Poster",
    "description": "The R package ‘orf’ is a software implementation of the Ordered Forest estimator as developed in Lechner and Okasa (2019). The Ordered Forest flexibly estimates the conditional class probabilities of models involving categorical outcomes with an inherent ordering structure, known as ordered choice models. Additionally to common machine learning algorithms, the Ordered Forest enables estimation of marginal effects together with statistical inference and thus provides comparable output as in standard econometric models. Accordingly, the ‘orf’ package provides generic R functions to estimate, predict, plot, print and summarize the estimation output of the Ordered Forest along with various options for specific forest-related tuning parameters. Finally, computational speed is ensured as the core forest algorithm relies on the fast C++ forest implementation from the ranger package (Wright and Ziegler 2017).",
    "email": "gabriel.okasa@unisg.ch"
  },
  {
    "title": "Power Supply health status monitoring dashboard",
    "author": "Marco Calderisi",
    "affiliation": "Kode srl, CTO",
    "namesurname": "MARCO,CALDERISI",
    "coauthor": "Jacopo Baldacci, Caterina Giacomelli, Ilaria Ceppa, Davide Massidda, Matteo Papi, Gabriele Galatolo, Francesca Giorgolo, ferdinando giordano, alessandro iovene",
    "track": "R Dataviz & Shiny",
    "session_type": "Poster",
    "description": "The Primis project dashboard allows to perform an analysis on the health status of power supplies boards on two levels:\r\n(1) analysis of a specific board, to check its status and the presence of any anomalies,\r\n(2) analysis of multiple boards within a single Power Supply, to check if the set of boards reveals abnormal behavior and if some boards behave in a distinctly different way from the others.\r\nThe analysis algorithms and the web application were created using the programming language R, and in particular the Shiny library.\r\nThe application is therefore divided into two parts that reflect these different types of analysis, called respectively \"Product View\" (analysis and diagnostics of a specific board) and \"Product Comparison\" (comparison analysis between multiple boards of the same Power Supply). Both analyzes can be carried out on an arbitrary time interval, selectable through a special application menu.\r\nThe analysis is carried out by means of: \r\n(1) univariate analysis, focusing on a specific parameter of one or more channels and displaying aggregate information regarding the status of the board in the entire observation period\r\n(2) multivariate analysis, that is the application of multivariate algorithms that allows to perform an overall analysis of the board, taking into account all the variables simultaneously.",
    "email": "m.calderisi@kode-solutions.net"
  },
  {
    "title": "First-year ICT students dropout predicting with R models",
    "author": "Natalja Maksimova",
    "affiliation": "Virumaa College of Tallinn University of Technology, lecturer",
    "namesurname": "NATALJA,MAKSIMOVA",
    "coauthor": "Olga Dunajeva",
    "track": "R Machine Learning & Models",
    "session_type": "Poster",
    "description": "The aim of this study is to find how it is possible to predict first-year ICT students dropout in one Estonian college, Virumaa College of Tallinn University of Technology (TalTech) and possibly to engage methods to decrease dropout rate. We perform three approaches of machine learning using R tools: logistic regressions, decision trees and Naive Bayes to predict. The models are computed on the basis of the TalTech study information system data. As a result, we propose a methodical approach that may be realized in practice at other institutions. \r\nAll applied methods yield high prediction with more than 85% accuracies. In the same time some influencing and non-influencing factors were found in predicting ICT students’ dropout.",
    "email": "natalja.maksimova@taltech.ee"
  },
  {
    "title": "Benchmark Percentage Disjoint Data Splitting in Cross Validation for Assessing the Skill of Machine",
    "author": "Olalekan Joseph Akintande",
    "affiliation": "University of Ibadan, Ph.D. Student",
    "namesurname": "OLALEKAN,JOSEPH,AKINTANDE",
    "coauthor": "O.E. Olubusoye",
    "track": "R Machine Learning & Models",
    "session_type": "Poster",
    "description": "The controversies surrounding dataset splitting technique and folklore of what has been or what should be, remain an open debate. Several authors (bloggers, researchers, and data scientists) in the field of machine learning and similar research areas, have proposed various arbitrary percentage disjoint dataset splitting (DDS) options for validating the skill of machine learning algorithms and by extension the appropriate percentage DDS based on cross-validation techniques. In this work, we propose benchmarks for which the percentage DDS procedure should be based. These benchmarks are founded on various training sizes (m) and serve as the basis and justification for the choice of an appropriate percentage DDS for assessing the skill of ML algorithms and related fields, on the concept of cross-validation techniques.",
    "email": "aojsoft@gmail.com"
  },
  {
    "title": "Integrating professional software engineering practices in medical research software",
    "author": "Patricia Ryser-Welch",
    "affiliation": "Newcastle University, Population Health Science Institute, Research Associate,",
    "namesurname": "PATRICIA,RYSER-WELCH",
    "coauthor": "http://www.datashield.ac.uk",
    "track": "R Applications",
    "session_type": "Poster",
    "description": "Health data sets are getting bigger, more complex, and are increasingly being linked up with other data sources. With this trend there is an increasing risk of patient identification and disclosure. Two different ways of mitigating this risk are to use a federated analysis approach or to use a data safe haven.\r\n\r\nDataSHIELD (www.datashield.ac.uk) is an established federated data analysis tool that is used in the medical sciences. This software has a variety of methods to reduce the risk of disclosure built in. Here we describe the steps we are taking to apply modern software engineering methodologies to DataSHIELD. The upcoming Medical Devices legislation requires that software has more rigourous testing done on it. While this legislation does not directly apply to software used for research, we think it is important the ideas behind this do filter down to research software. For us these principles include testing that functions work, as well as testing that they produce the correct answers. Using a static standard data set to test against (that is publicly available) is also an important aspect. This work is being done in a continuous integraion framework using Microsoft Azure. Additionally all our software is developed as open source.\r\n\r\nIn addition to the protection DataSHIELD provides on its own we are also integrating it into our Trusted Research Environment as part of Connected Health Cities North East and North Cumbria. This will give an extra level of protection to data that may automatically flow from multiple data sources. Additionally, as analysis can be done in a federated way it means that that data does not need to leave its data controller's environment. This opens up the possibility of analysis happening across trusts and regions.",
    "email": "patricia.ryser-welch@newcastle.ac.uk"
  },
  {
    "title": "Dealing with changing administrative boundaries: The case of Swiss municipalities",
    "author": "Tobias Schieferdecker",
    "affiliation": "Daily dealings with data at cynkra",
    "namesurname": "TOBIAS,SCHIEFERDECKER",
    "coauthor": "Thomas Knecht, Kirill Müller, Christoph Sax",
    "track": "R Applications",
    "session_type": "Poster",
    "description": "Switzerland's municipalities are frequently merged or reorganized, in an attempt to reduce costs and increase efficiency.\r\nThese mergers create a substantial problem for data analysis.\r\nOften it is desirable to study a municipality over time.\r\nBut in order to create a time series for a region of interest, its borders should stay constant.\r\nOur goal is to provide R-functions that allow an easy and consistent handling of these mergers.\r\nWe create a mapping table for municipality ID’s for a specified period of time, that allows us to track the mergers over time.\r\nWe also provide weights, such as population as well as area of the municipalities, to facilitate the construction of weighted time series.\r\nVarious other municipality mutations are also taken into account.\r\n\r\nWe are creating two R packages: an infrastructure package that handles the task of keeping the data up to date; and a user package that contains the functions to deal with the mergers.",
    "email": "tobias@cynkra.com"
  },
  {
    "title": "badDEA: An R package for measuring firms’ efficiency adjusted by undesirable outputs",
    "author": "Yann Desjeux",
    "affiliation": "INRAE, France",
    "namesurname": "YANN,DESJEUX",
    "coauthor": "K Hervé DAKPO; Yann DESJEUX; Laure LATRUFFE",
    "track": "R Applications",
    "session_type": "Poster",
    "description": "Growing concerns on the detrimental effects of human production activities on the environment, e.g. air, soil and water pollution, have triggered the development of new performance indicators (including productivity and efficiency measures) accounting for such undesirable impacts. Firms can now be benchmarked not only in terms of economic performance, but also in terms of environmental performance linked to production. In the performance benchmarking literature, and more specifically the one on the non-parametric approach Data Envelopment Analysis (DEA), several methodologies have been developed to consider these impacts as undesirable (or bad) outputs. Related empirical applications in the literature, performed with various software, show that conclusions differ depending on the way these undesirable outputs are introduced. However none of these methodologies are routinely developed in R. In this context, we developed the badDEA package in order to provide a consistent and single framework where users (students, researchers, practitioners) can find the major methodologies proposed in the literature to compute efficiency measures that are adjusted by undesirable outputs. In this presentation, we will describe the aim, structure and options of the badDEA package, unfolding all the methodologies in their different variants and providing a promising tool for decision-making.",
    "email": "yann.desjeux@inrae.fr"
  },
  {
    "title": "Design Patterns For Big Shiny Apps",
    "author": "Alex Gold",
    "affiliation": "Solutions Engineer, RStudio",
    "namesurname": "ALEX,GOLD",
    "coauthor": "Cole Arendt",
    "track": "R Dataviz & Shiny, R Production",
    "session_type": "Regular talk",
    "description": "In about 20 minutes on the morning of January 27, 2020, one engineer launched over 500 individual cloud server instances for workshop attendees at RStudio::conf and managed them for the duration of the workshops — all from a Shiny app. The RStudio team manages a variety of production systems using Shiny apps including our workshop infrastructure and access to our sales demo server. \r\n\r\nThe Shiny apps are robust enough for these mission-critical activities because of an important lesson from web engineering: separation of concerns between front-end user interaction logic and back-end business logic. This design pattern can be implemented in R by creating user interfaces in Shiny and managing interactions with other systems with Plumber APIs and R6 classes. \r\n\r\nThis pattern allows for even complex Shiny apps to still be understandable and maintainable. Moreover, this pattern of designing and building large Shiny apps is broadly applicable to any app that has substantial interaction with outside systems. Session attendees will gain an understanding of this pattern, which can be useful for many large Shiny apps.",
    "email": "alexkgold@gmail.com"
  },
  {
    "title": "Using XGBoost, Plumber and Docker in production to power a new banking product",
    "author": "André Rivenæs",
    "affiliation": "Data Scientist, PwC",
    "namesurname": "ANDRÉ,RIVENÆS",
    "author2": "Markus Mortensen",
    "affiliation2": "PwC",
    "track": "R Machine Learning & Models, R Production",
    "session_type": "Regular talk",
    "description": "Buffer is a brand new and innovative banking product by one of the largest retail banks in Norway, Sparebanken Vest, and it is powered by R.\r\n\r\nIn fact, the product's decision engine is written entirely in R. We analyze whether a customer should get a loan and how much loan they should be allocated by analyzing large amounts of data from various sources. An essential part is analyzing the customer's invoices using machine learning (XGBoost). \r\n\r\nIn this talk, we will cover:\r\n\r\n- How we use ML and Bayesian statistics to estimate the probability of an invoice being repaid. \r\n- How we successfully put the decision engine in production, using e.g. Plumber, Docker, CircleCI and Kubernetes. \r\n- What we have learned from using R in production at scale.",
    "email": "andre@rivenaes.net"
  },
  {
    "title": "Astronomical source detection and background separation: a Bayesian nonparametric approach",
    "author": "Andrea Sottosanti",
    "affiliation": "University of Padova",
    "namesurname": "ANDREA,SOTTOSANTI",
    "coauthor": "Mauro Bernardi, Alessandra R. Brazzale, Roberto Trotta, David A. van Dyk",
    "track": "R Machine Learning & Models, R Applications",
    "session_type": "Regular talk",
    "description": "We propose an innovative approach based on Bayesian nonparametric methods to the signal extraction of astronomical sources in gamma-ray count maps under the presence of a strong background contamination. Our model simultaneously induces clustering on the photons using their spatial information and gives an estimate of the number of sources, while separating them from the irregular signal of the background component that extends over the entire map. From a statistical perspective, the signal of the sources is modeled using a Dirichlet Process mixture, that allows to discover and locate a possible infinite number of clusters, while the background component is completely reconstructed using a new flexible Bayesian nonparametric model based on b-spline basis functions. The resultant can be then thought of as a hierarchical mixture of nonparametric mixtures for flexible clustering of highly contaminated signals. We provide also a Markov chain Monte Carlo algorithm to infer on the posterior distribution of the model parameters, and a suitable post-processing algorithm to quantify the information coming from the detected clusters. Results on different datasets confirm the capacity of the model to discover and locate the sources in the analysed map, to quantify their intensities and to estimate and account for the presence of the background contamination.",
    "email": "sottosanti@stat.unipd.it"
  },
  {
    "title": "High dimensional sampling and volume computation",
    "author": "Apostolos Chalkis",
    "affiliation": "PhD in Computer Science",
    "namesurname": "APOSTOLOS,CHALKIS",
    "coauthor": "Vissarion Fisikopoulos",
    "track": "R Machine Learning & Models",
    "session_type": "Regular talk",
    "description": "Sampling from multivariate distributions is a fundamental problem in statistics that plays important role in modern machine learning and data science. Many important problems such as convex optimization and multivariate integration can be efficiently solved via sampling. This talk presents the CRAN package volesti which offers to R community efficient C++ implementations of state-of-the-art algorithms for sampling and volume computation of convex sets. It scales up to hundred or thousand dimensions, depending the problem, providing the most efficient implementations for sampling and volume computation to date. Thus, volesti allows users to solve problems in dimensions and order of magnitude higher than before. We present the basic functionality of volesti and show how it can be used to provide approximate solutions to intractable problems in combinatorics, financial modeling, bioinformatics and engineering. We stand out two famous applications in finance. We show how volesti can be used to detect financial crises and evaluate portfolios performance in large stock markets with hundreds of assets, by giving real life examples using public data.",
    "email": "tolis.chal@gmail.com"
  },
  {
    "title": "Fake News: AI on the battle ground",
    "author": "Ayomide Shodipo",
    "affiliation": "Senior Developer Advocate & Media Developer Expert at Cloudinary",
    "namesurname": "AYOMIDE,SHODIPO",
    "track": "R Machine Learning & Models, R Life Sciences, R Production, R World",
    "session_type": "Regular talk",
    "description": "Assumed products have been a longstanding and growing pain for companies around the globe. In addition to impacting company revenue, they damage brand reputation and customer confidence. Companies were asked to build a solution for a global electronics brand that can identify fake products by just taking one picture on a smartphone.\r\n\r\nIn this session, we will look into the building blocks that make this AI solution work. We’ll find out that there is much more to it than just training a convolutional neural network.\r\n\r\nWe look at challenges like how to manage and monitor the AI model and how to build and improve the model in a way that fits your DevOps production chain.\r\n\r\nLearn how we used Azure Functions, Cosmos DB and Docker to build a solid foundation. See how we used the Azure Machine Learning service to train the models. And find out how we used Azure DevOps to control, build and deploy this state-of-the-art solution.",
    "email": "shodipovi@gmail.com"
  },
  {
    "title": "From consulting to open-source and back",
    "author": "Christoph Sax",
    "affiliation": "R-enthusiast, economist @cynkra",
    "namesurname": "CHRISTOPH,SAX",
    "coauthor": "Kirill Müller",
    "track": "R World",
    "session_type": "Regular talk",
    "description": "Open-source development is a great source of satisfaction and fulfillment, but someone has to pay the bills. A straightforward solution is to consult customers and help them to pick the right tools. As a small group of R enthusiasts, we try to align open source development by supporting our clients to accomplish their goals, contributing to the community along the way. It turns out that the benefits work in both ways: In addition to funding, consulting work allows us to test our tools and to improve their usability in a practical setting. At the same time, the involvement in open source development sharpens our analytical skills and serves as a first stop for new customers. Ideally, consulting projects lead to new developments, which in turn lead to new consulting projects.",
    "email": "christoph@cynkra.com"
  },
  {
    "title": "Deduplicating real estate ads using Naive Bayes record linkage",
    "author": "Daniel Meister",
    "affiliation": "Datahouse AG",
    "namesurname": "DANIEL,MEISTER",
    "track": "R Applications",
    "session_type": "Regular talk",
    "description": "We demonstrate in this talk, how we used a containerized R and PostgreSQL data pipeline to deduplicate 60 million real estate ads from Germany and Switzerland using a multi-step Naive Bayes record linkage approach. Real estate platforms publish millions of rental flat and condominium ads yearly. A given region or country of interest is normally covered by various competing platforms, leading to multiple published ads for a single real world object. Because quantifying and modeling the real estate market requires unbiased input data, our aim was to deduplicate real estate ads using Naive Bayes record linkage. We used commercially available German and Swiss real estate ad data from 2012 to 2019 consisting of approximately 60 million individual records. After multiple data cleaning and preparation steps we employed a Naive Bayes weighting of 12-14 variables to calculate similarity scores between ads and determined a linkage threshold based on expert judgment. The deduplication pipeline consisted of three steps: linking ads based on identity comparisons, linking similar ads within small regional areas (municipalities) and linking similar ads within large regional areas (cantons, states). The pipeline was deployed as a containerized setup with in-memory calculations in R and out-of-memory calculations and data storage in PostgreSQL. Deduplication linked the around 60 million ads to around 14 million object groups (Germany: 10 millions, Switzerland: 4 millions). The distribution of similarity scores showed high separation power and the resulting object groups displayed high homogeneity in geographic location and price distribution. Furthermore, yearly results corresponded well with published relocation rates. Using Naive Bayes record linkage to deduplicate real estate ads resulted in a sensible grouping of ads into object groups (rental flats, condominiums). We were able to combine similarities across different variables into a single similarity score. An advantage of the Naive Bayes approach is the high interpretability of the influence of individual variables. However, by manually determining the linkage threshold our results are heavily influenced by possible expert biases. The containerized R and PostgreSQL setup proved it’s portability and scaling capabilities. The same approach could easily be transferred to other domains requiring deduplication of multivariate data sets.",
    "email": "daniel.meister@datahouse.ch"
  },
  {
    "title": "{polite}: web etiquette for R users",
    "author": "Dmytro Perepolkin",
    "affiliation": "Lund University",
    "namesurname": "DMYTRO,PEREPOLKIN",
    "track": "R World, R Applications",
    "session_type": "Regular talk",
    "description": "Data is everywhere, but it does not mean it is freely available. What are best practices and acceptable norms for accessing the data on the web? How does one know when it is OK to scrape the content of a website and how to do it in such a way that it does not create problems for data owner and/or other users? This talk with provide examples of using {polite} package for safe and responsible web scraping. The three pillars of {polite} are seeking permission, taking slowly and never asking twice.",
    "email": "dperepolkin@gmail.com"
  },
  {
    "title": "Hydrological Modelling and R",
    "author": "Emanuele Cordano",
    "affiliation": "www.rendena100.eu",
    "namesurname": "EMANUELE,CORDANO",
    "coauthor": "Giacomo Bertoldi",
    "track": "R Applications",
    "session_type": "Regular talk",
    "description": "Eco-hydrological and biophysical models are increasingly used in the contexts of hydrology, ecology, precision agriculture for better management of water resources and climate change impact studies at various scales: local, watershed or regional scale. However, to satisfy the researchers and stakeholders demand, user friendly interfaces are needed. The integration of such models in the powerful software environment of R greatly eases the application, input data preparation, output elaboration and visualization. In this work we present new developments for a R interface (R open-source package **geotopbricks** (https://CRAN.R-project.org/package=geotopbricks) and related) for the GEOtop hydrological distributed model (www.geotop.org - GNU General Public License v3.0). This package aims to be a link between the work of environmental engineers, who develop hydrological models, and the ones of data and applied scientists, who can extract information from the model results. Applications related to the simulation of water cycle dynamics (model calibration, mapping, data visualization) in some alpine basins and under scenarios of climate change and variability are shown. In particular, we will present an application to predict with the model winter snow conditions, which play a critical role in governing the spatial distribution of fauna in temperate ecosystems.",
    "email": "emanuele.cordano@gmail.com"
  },
  {
    "title": "GeneTonic: enjoy RNA-seq data analysis, responsibly",
    "author": "Federico Marini",
    "affiliation": "Center for Thrombosis and Hemostasis (CTH) & Institute of Medical Biostatistics, Epidemiology and Informatics (IMBEI) - University Medical Center Mainz",
    "namesurname": "FEDERICO,MARINI",
    "track": "R Life Sciences",
    "session_type": "Regular talk",
    "description": "Interpreting the results from RNA-seq transcriptome experiments can be a complex task, where the essential information is distributed among different tabular and list formats - normalized expression values, results from differential expression analysis, and results from functional enrichment analyses.\r\n\r\nThe identification of relevant functional patterns, as well as their contextualization in the data and results at hand, are not straightforward operations if these pieces of information are not combined together efficiently.\r\n\r\nInteractivity can play an essential role in simplifying the way how one accesses and digests RNA-seq data analysis in a more comprehensive way.\r\n\r\nI introduce `GeneTonic` (https://github.com/federicomarini/GeneTonic), an application developed in Shiny and based on many essential elements of the Bioconductor project, that aims to reduce the barrier to understanding such data better, and to efficiently combine the different components of the analytic workflow.\r\n\r\nFor example, starting from bird's eye perspective summaries (with interactive bipartite gene-geneset graphs, or enrichment maps), it is easy to generate a number of visualizations, where drill-down user actions enable further insight and deliver additional information (e.g., gene info boxes, geneset summary, and signature heatmaps).\r\n\r\nComplex datasets interpretation can be wrapped up into a single call to the GeneTonic main function, which also supports built-in RMarkdown reporting, to both conclude an exploration session, or also to generate in batch the output of the available functionality, delivering an essential foundation for computational reproducibility.",
    "email": "marinif@uni-mainz.de"
  },
  {
    "title": "A simple and flexible inactivity/sleep detection R package",
    "author": "Francesca Giorgolo",
    "affiliation": "Kode s.r.l. - Data Scientist",
    "namesurname": "FRANCESCA,GIORGOLO",
    "coauthor": "Ilaria Ceppa, Marco Calderisi, Davide Massidda, Matteo Papi, Gabriele Galatolo, Andrea Spinelli, Andrea Zedda, Jacopo Baldacci, Caterina Giacomelli",
    "track": "R Life Sciences",
    "session_type": "Regular talk",
    "description": "With the widespread usage of wearable devices great amount of data became available and new fields of application arised, like health monitoring and activity detection. \r\nOur work focused on inactivity and sleep detection from continuous raw tri-axis accelerometer data, recorded using different accelerometers brands having sampling frequencies below and above 1Hz.\r\nThe algorithm implemented is the SPT-window detection algorithm described in literature slighty modified to met the flexibility requirement we imposed ourselves.\r\nThe R package developed provides functions to clean data, to identify inactivity/sleep windows and to visualize the results.\r\nThe main function has a parameter to specify the measurement unit of the data, a threshold to distinguish low and high activity and also a parameter to handle non-wearing periods, where a non wear period is defined as a period of time where all the accelerometers are equal to zero. Other functions allow to separate overlapped accelerometer signals, i.e. when a device is replaced by another, and to visualize the obtained results.",
    "email": "f.giorgolo@kode-solutions.net"
  },
  {
    "title": "progressr: An Inclusive, Unifying API for Progress Updates",
    "author": "Henrik Bengtsson",
    "affiliation": "UCSF, Assoc Prof, CS/Stats, R since 2000",
    "namesurname": "HENRIK,BENGTSSON",
    "track": "R Production, R Applications",
    "session_type": "Regular talk",
    "description": "The 'progressr' package provides a minimal, unifying API for scripts and packages to report progress from anywhere including when using parallel processing to anywhere.\r\n\r\nIt is designed such that the developer can focus on what to report progress on without having to worry about how to present it. The end user has full control of how, where, and when to render these progress updates. Progress bars from popular progress packages are supported and more can be added.\r\n\r\nThe 'progressr' is inclusive by design. Specifically, no assumptions are made how progress is reported, i.e. it does not have to be a progress bar in the terminal. Progress can also be reported as audio (e.g. unique begin and end sounds with intermediate non-intrusive step sounds), or via a local or online notification system.\r\n\r\nAnother novelty is that progress updates are controlled and signaled via R's condition framework. Because of this, there is no need for progress-specific arguments and progress can be reported from nearly everywhere in R, e.g. in classical for and while loops, within map-reduce APIs like the 'lapply()' family of functions, 'purrr', 'plyr', and 'foreach'. It also works with parallel processing via the 'future' framework, e.g. 'future.apply', 'furrr', and 'foreach' with 'doFuture'. The package is compatible with Shiny applications.",
    "email": "henrik.bengtsson@gmail.com"
  },
  {
    "title": "varycoef: Modeling Spatially Varying Coefficients",
    "author": "Jakob Dambon",
    "affiliation": "HSLU & UZH, Switzerland",
    "namesurname": "JAKOB,DAMBON",
    "coauthor": "Fabio Sigrist, Reinhard Furrer",
    "track": "R Machine Learning & Models",
    "session_type": "Regular talk",
    "description": "In regression models for spatial data, it is often assumed that the marginal effects of covariates on the response are constant over space. In practice, this assumption might often be questionable. Spatially varying coefficient (SVC) models are commonly used to account for spatial structure within the coefficients. \r\nWith the R package varycoef, we provide the frame work to estimate Gaussian process-based SVC models. It is based on maximum likelihood estimation (MLE) and in contrast to existing model-based approaches, our method scales better to data where both the number of spatial points is large and the number of spatially varying covariates is moderately-sized, e.g., above ten.\r\nWe compare our methodology to existing methods such as a Bayesian approach using the stochastic partial differential equation (SPDE) link, geographically weighted regression (GWR), and eigenvector spatial filtering (ESF) in both a simulation study and an application where the goal is to predict prices of real estate apartments in Switzerland. The results from both the simulation study and application show that our proposed approach results in increased predictive accuracy and more precise estimates.",
    "email": "jakob.dambon@gmail.com"
  },
  {
    "title": "FastAI in R: preserving wildlife with computer vision",
    "author": "Jędrzej Świeżewski",
    "affiliation": "Data Scientist at Appsilon",
    "namesurname": "JĘDRZEJ,ŚWIEŻEWSKI",
    "coauthor": "Marek Rogala",
    "track": "R Machine Learning & Models",
    "session_type": "Regular talk",
    "description": "In this presentation, we will discuss using the latest techniques in computer vision as an important part of “AI for Good” efforts, namely, enhancing wildlife preservation. We will present how to make use of the latest technical advancements in an R setup even if they are originally implemented in Python.\r\n\r\nA topic rightfully receiving growing attention among Machine Learning researchers and practitioners is how to make good use of the power obtained with the advancement of the tools. One of the avenues in these efforts is assisting wildlife conservation by employing computer vision in making observations of wildlife much more effective. We will discuss several of such efforts during the talk.\r\n\r\nOne of the very promising frameworks for computer vision developed recently is the Fast.ai wrapper of PyTorch, a Python framework used for computer vision among other things. While it incorporates the latest theoretical developments in the field (such as one cycle policy training) it provides an easy to use framework allowing a much wider audience to benefit from the tools, such as AI for Good initiatives run by people who are not formally trained in Machine Learning.\r\n\r\nDuring the presentation we will show how to make use of a model trained using the Python’s fastai library within an R workflow with the use of the reticulate package. We will focus on use cases concerning classifying species of African wildlife based on images from camera traps.",
    "email": "jedrzej@appsilon.com"
  },
  {
    "title": "Powering Turing e-Atlas with R",
    "author": "Layik Hama",
    "affiliation": "Alan Turing Institute",
    "namesurname": "LAYIK,HAMA",
    "coauthor": "Dr Nik Lomax, Dr Roger Beecham",
    "track": "R Applications, R Production, R Dataviz & Shiny",
    "session_type": "Regular talk",
    "description": "Turing e-Atlas is a research project under the Urban Analytics research theme at Alan Turing Institute (ATI). The ATI is UK's national institute for data science and Artificial Intelligence based at the British Library in London. \r\n\r\nThe research is a grand vision for which we have been trying to take baby steps under the banner of an e-Atlas. And we believe R is positioned to play a foundation role in any scalable solution to analyse and visualize large scale datasets especially geospatial datasets. \r\n\r\nThe application presented is built using RStudio's Plumber package which relies on solid libraries to develop web applications. The front-end is made up of Uber's various visualization packages using Facebook's React JavaScript framework.",
    "email": "l.hama@leeds.ac.uk"
  },
  {
    "title": "Using process mining principles to extract a collaboration graph from a version control system log",
    "author": "Leen JOOKEN",
    "affiliation": "Hasselt University, PhD student",
    "namesurname": "LEEN,JOOKEN",
    "coauthor": "Gert Janssenswillen, Mathijs Creemers, Mieke Jans, Benoît Depaire",
    "track": "R Production, R World",
    "session_type": "Regular talk",
    "description": "Knowledge management is an indispensable component of modern-day, fast changing and flexible software engineering environments. A clear overview on how software developers collaborate can reveal interesting insights such as the general structure of collaboration, crucial resources, and risks in terms of knowledge preservation that can arise when a programmer decides to leave the company. Version control system (VCS) logs, which keep track of what team members work on and when, contain data to provide these insights. We present an R package that provides an algorithm which extracts and visualizes a collaboration graph from VCS log data. The algorithm is based on principles from graph theory, cartography and process mining. Its structure consists of four phases: (1) building the base graph, (2) calculating weights for nodes and edges, (3) simplifying the graph using aggregation and abstraction, and (4) extending it to include specific insights of interest. Each of these phases offers the user a lot of flexibility in deciding which parameters and metrics to include. This makes it possible for the human expert to exploit his existing knowledge about the project and team to guide the algorithm in building the graph that best fits his specific use case, and hence will provide the most accurate insights.",
    "email": "leen.jooken@uhasselt.be"
  },
  {
    "title": "Manifoldgstat: an R package for spatial statistics of manifold data",
    "author": "Luca Torriani",
    "affiliation": "MOX, Department of Mathematics, Politecnico di Milano",
    "namesurname": "LUCA,TORRIANI",
    "coauthor": "Alessandra Menafoglio, Piercesare Secchi",
    "author2": "Ilaria Sartori",
    "affiliation2": "Politecnico di Milano",
    "track": "R Machine Learning & Models",
    "session_type": "Regular talk",
    "description": "The statistical analysis of data belonging to Riemannian manifolds is becoming increasingly important in many applications, such as shape analysis or diffusion tensor imaging. In many cases, the available data are georeferenced, making spatial dependence a non-negligible data characteristic. Modeling and accounting for it, typically, is not trivial, because of the non-linear geometry of the manifold. In this contribution, we present the Manifoldgstat R package, which provides a set of fast routines allowing to efficiently analyze sets of spatial Riemannian data, based on state-of-the-art statistical methodologies. The package stems from the need to create an efficient and reproducible environment allowing to run extensive simulation studies and bagging algorithms for spatial prediction of symmetric positive definite matrices. The package implements three main algorithms (Pigoli et al, 2016, Menafoglio et al, 2019, Sartori & Torriani, 2019). The latter two are particularly computationally demanding, as they rely on Random Domain Decompositions of the geographical domain. To substantially improve performances, the package exploits dplyr and Rcpp to integrate R with C++ code, where template factories handle all run-time choices. In this communication, we shall revise the characteristics of the three methodologies considered, and the key-points of their implementation.",
    "email": "luca.torriani94@gmail.com"
  },
  {
    "title": "Voronoi Linkage for Spatially Misaligned Data",
    "author": "Luís G. Silva e Silva",
    "affiliation": "Food and Agriculture Organization - FAO - Data Scientist",
    "namesurname": "LUÍS,G.,SILVA,E,SILVA",
    "coauthor": "Lucas Godoy, Douglas Azevedo, Augusto Marcolin, Jun Yan",
    "track": "R Dataviz & Shiny, R World",
    "session_type": "Regular talk",
    "description": "In studies of elections, voting outcomes are point-referenced at voting stations while socioeconomic covariates are areal data available at census tracts. The misaligned spatial structure of these two data sources makes the regression analysis to identify socioeconomic factors that affect the voting outcomes a challenge. Here we propose a novel approach to link these two sources of spatial data through Voronoi tessellation. Our proposal is creating a Voronoi tessellation with respect to the point-referenced data, with this outset, the spatial points become a set of mutually exclusive polygons named Voronoi cells. The extraction of data from the census tracts is proportional to the intersection area of each census tract polygon and Voronoi cells. Consequently, we use 100% of the available information and preserve the polygons’ autocorrelation structure. When visualised through our Shiny App, the method provides a finer spatial resolution than municipalities and facilitates the identification of spatial structures at a most detailed level. The technique is applied for the 2018 Brazilian presidential election data. The tool provides deep access to Brazilian election results by enabling to create general maps, plots, and tables by states and cities.",
    "email": "lgsilvaesilva@gmail.com"
  },
  {
    "title": "Be proud of your code! Tools and patterns for making production-ready, clean R code",
    "author": "Marcin Dubel",
    "affiliation": "Software Engineer at Appsilon Data Science",
    "namesurname": "MARCIN,DUBEL",
    "track": "R Production, R World",
    "session_type": "Regular talk",
    "description": "In this talk you’ll learn the tools and best practices for making clean, reproducible R code in a working environment ready to be shared and productionalised. Save your time for maintenance, adjusting and struggling with packages. \r\n\r\nR is a great tool for fast data analysis. It’s simplicity in setup combined with powerful features and community support makes it a perfect language for many subject matter experts e.g. in finance or bioinformatics. Yet often what started as a pet project or proof of concept begins to grow and expand, with additional collaborators working on it. It is then crucial that you have your project organised well, reusable, with an environment set, so that the code works every time and on any machine. Otherwise the solution won’t be used by anyone but you. By following a few patterns and with appropriate tools it won’t be overwhelming or disturbing and will highlight the true value of the code.\r\n\r\nBoth Appsilon and I personally have taken part in many R projects for which the goal was to clean and organise the code as well as the project structure. We would like to share our experience, best practices and useful tools to share code shamelessly.\r\n\r\nDuring the presentation I will show: \r\nsetting up the development environment with **packrat**, **renv** and **docker**,\r\norganising the project structure,\r\nthe best practices in writing R code, automated with **linter**,\r\nsharing the code using git,\r\norganising workflow with **drake**,\r\noptimising the Shiny apps and data loading with **plumber** and **database**,\r\npreparing the tests and continuous integration **circle CI**.",
    "email": "marcin@appsilon.com"
  },
  {
    "title": "Going in the fast lane with R. How we use R within the biggest digital dealer program in EMEA.",
    "author": "Marco Cavaliere",
    "affiliation": "Like Reply - Business Data Analyst",
    "namesurname": "MARCO,CAVALIERE",
    "track": "R Production, R Machine Learning & Models",
    "session_type": "Regular talk",
    "description": "How we are using R as the foundation of all the data-related tasks in the biggest dealer digital program at FCA. \r\nFrom simple tasks as dashboarding or reporting to more strategic capabilities as predicting advertising ROI through Tensorflow or developing a production-grade, data-driven microservices, we leverage R ecosystem to deliver better results and increase the data awareness for all the project stakeholders.",
    "email": "m.cavaliere@reply.it"
  },
  {
    "title": "R alongside Airflow, Docker and Gitlab CI",
    "author": "Matthias Bannert",
    "affiliation": "Research Engineering Lead at ETH Zurich, KOF Swiss Economic Institute",
    "namesurname": "MATTHIAS,BANNERT",
    "track": "R Production",
    "session_type": "Regular talk",
    "description": "The KOF Swiss Economic Institute at ETH Zurich (Switzerland) regularly surveys more than 10K companies, computes economic indicators and forecasts as it monitors the national economy. Today, the institute updates its website in automated fashion and operates automated data pipelines to partners such as regional statistical offices or the Swiss National Bank. At KOF, production is based on an open source ecosystem to a large degree. More and more processes are being migrated to an environment that consists of open source components Apache Airflow, Docker, Gitlab Continous Integration, PostgreSQL and R. This talk shows not only how well R interfaces and works with all parts from workflow automation to databases, but also how R's advantages impact this system: From R Studio Servers to internal packages and an own internal mini-CRAN, the use of the R language is crucial in making the environment stable and convenient to maintain with the software carpentry type of background.",
    "email": "matthias.bannert@gmail.com"
  },
  {
    "title": "DaMiRseq 2.0: from high dimensional data to cost-effective reliable prediction models",
    "author": "Mattia Chiesa",
    "affiliation": "Senior data scientist @ Centro Cardiologico Monzino IRCCS",
    "namesurname": "MATTIA,CHIESA",
    "coauthor": "Chiara Vavassori, Gualtiero I. Colombo, Luca Piacentini",
    "track": "R Life Sciences",
    "session_type": "Regular talk",
    "description": "High dimensional data generated by modern high-throughput platforms pose a great challenge in selecting a small number of informative variables, for biomarker discovery and classification. Machine learning is an appropriate approach to derive general knowledge from data, identifying highly discriminative features and building accurate prediction models. To this end, we developed the R/Bioconductor package DaMiRseq, which (i) helps researchers to filter and normalize high dimensional datasets, arising from RNA-Seq experiments, by removing noise and bias and (ii) exploits a custom machine learning workflow to select the minimum set of robust informative features able to discriminate classes.\r\nHere, we present the version 2.0 of the DaMiRseq package, an extension that provides a flexible and convenient framework for managing high dimensional data such as omics data, large-scale medical histories, or even social media and financial data. Specifically, DaMiRseq 2.0 implements new functions that allow training and testing of several different classifiers and selection of the most reliable one, in terms of classification performance and number of selected features. The resulting classification model can be further used for any prediction purpose. This framework will give users the ability to build an efficient prediction model that can be easily replicated in further related settings.",
    "email": "mattia.chiesa@ccfm.it"
  },
  {
    "title": "How to apply R in a hospital environment on standard available hospital-wide data",
    "author": "Mieke Deschepper",
    "affiliation": "University hospital Ghent, staf member Strategic Policy cell, Ph.D.",
    "namesurname": "MIEKE,DESCHEPPER",
    "track": "R Life Sciences",
    "session_type": "Regular talk",
    "description": "Lots of data is registered within hospitals, for financial, clinical and administrative purposes. Today, this data is barely used. Due to not knowing the existence of the data, the possible applications and the skills to execute the analysis, …\r\nIn this presentation we show how we can apply R on this data and what the possibilities are using standard available hospital-wide data on a low cost budget. \r\n1.\tReporting with R \r\n-\tusing R and markdown as a tool for management reporting\r\n-\tUsing R for data handling (ETL)\r\n-\tShiny applications as alternative for dashboarding\r\n2.\tUsing R as a statistical tool\r\n-\tPerforming regression models to gain insight in certain predictor\r\n3.\tUsing R a data science tool\r\n-\tUsing R to perform Machine Learning analysis, e.g. Random Forests\r\n-\tUsing R for the data wrangling and handle the high dimensional data\r\n4.\tRequirements for all of the above",
    "email": "mieke.deschepper@uzgent.be"
  },
  {
    "title": "Computer Algebra Systems in R",
    "author": "Mikkel Meyer Andersen",
    "affiliation": "Assoc. Prof., Department of Mathematical Sciences, Aalborg University, Denmark",
    "namesurname": "MIKKEL,MEYER,ANDERSEN",
    "coauthor": "Søren Højsgaard",
    "track": "R World, R Machine Learning & Models, R Applications",
    "session_type": "Regular talk",
    "description": "R's ability to do symbolic mathematics is largely restricted to finding derivatives. There are many tasks involving symbolic math and that are of interest to R users, e.g. inversion of symbolic matrices, limits and solving non-linear equations. Users must resort to other computer algebra systems (CAS) for such tasks and many R users (especially outside of academia) do not readily have access to such software. There are also other indirect use-cases of symbolic mathematics in R that can exploit other strengths of R, including Shiny apps with auto-generated mathematics exercises.\r\n\r\nWe are maintaining two packages enabling symbolic mathematics in R: Ryacas and caracas. Ryacas is based on Yacas (Yet Another Computer Algebra System) and caracas is based on SymPy (Python library). Each have their advantages: Yacas is extensible and has a close integration to R which makes auto-generated mathematics exercises easy to make. SymPy is feature-rich and thus gives many possibilities.\r\n\r\nIn this talk we will discuss the two packages and demonstrate various use-cases including uses that help understanding statistical models and Shiny apps with auto-generated mathematics exercises.",
    "email": "mikl@math.aau.dk"
  },
  {
    "title": "Interpretable and accessible Deep Learning for omics data with R and friends",
    "author": "Moritz Hess",
    "affiliation": "Research Associate, Institute of Medical Biometry and Statistics, Faculty of Medicine and Medical Center - University of Freiburg",
    "namesurname": "MORITZ,HESS",
    "coauthor": "Stefan Lenz, Harald Binder",
    "track": "R Life Sciences",
    "session_type": "Regular talk",
    "description": "Recently, generative Deep Learning approaches were shown to have a huge potential for e.g. retrieving compact, latent representations of high-dimensional omics data such as single-cell RNA-Seq data. However, there are no established methods to infer how these latent representations relate to the observed variables, i.e. the genes.\r\n\r\nFor extracting interpretable patterns from gene expression data that indicate distinct sub-populations in the data, we here employ log-linear models, applied to the synthetic data and corresponding latent representations, sampled from generative deep models, which were trained with single-cell gene expression data.\r\n\r\nWhile omics data are routinely analyzed in R and powerful toolboxes, tailored to omics data are available, there are no established and truely accessible approaches for Deep Learning applications here. \r\n\r\nTo close this gap, we here demonstrate how easily customizable Deep Learning frameworks, developed for the Julia programming language, can be leveraged in R, to perform accessible and interpretable Deep Learning with omics data.",
    "email": "hess@imbi.uni-freiburg.de"
  },
  {
    "title": "Elevating shiny module with {tidymodules}",
    "author": "Mustapha Larbaoui",
    "affiliation": "Novartis, Associate Director, Scientific Computing & Consulting",
    "namesurname": "MUSTAPHA,LARBAOUI",
    "coauthor": "Doug Robinson, Xiao Ni, David Granjon",
    "track": "R Dataviz & Shiny",
    "session_type": "Regular talk",
    "description": "Shiny App developers have warmly welcomed the concept of Shiny modules as a way to simplify the app development process through the introduction of reusable building blocks. Shiny modules are similar in spirit to the concept of functions in R, except each is implemented with paired ui and server codes along with their own namespace. The {tidymodules} R package introduces a novel structure that harmonizes module development based on R6 (https://r6.r-lib.org/), which is an implementation of encapsulated object-oriented programming for R, thus knowledge of R6 is a prerequisite for using {tidymodules} to develop Shiny modules. Some key features of this package are module encapsulation, reference semantics, central module store and an innovative framework for enabling and facilitating cross module – module communication. It does this through the creation of “ports”, both input and output, where users may pass data and information through pipe operators. Because the connections are strictly specified, the module network may be visualized which shows how data move from one module to another. We feel the {tidymodules} framework will simplify the module development process and will reduce the code complexity through programming concepts like inheritance.",
    "email": "mustapha.larbaoui@novartis.com"
  },
  {
    "title": "APFr: Average Power Function and Bayes FDR for Robust Brain Networks Construction",
    "author": "Nicolo' Margaritella",
    "affiliation": "University of Edinburgh",
    "namesurname": "NICOLO',MARGARITELLA",
    "coauthor": "Piero Quatto",
    "track": "R Life Sciences",
    "session_type": "Regular talk",
    "description": "Brain functional connectivity is widely investigated in neuroscience. In recent years, the study of brain connectivity has been largely aided by graph theory. The link between time series recorded at multiple locations in the brain and a graph is usually an adjacency matrix. This converts a measure of the connectivity between two time series, typically a correlation coefficient, into a binary choice on whether the two brain locations are functionally connected or not. As a result, the choice of a threshold over the correlation coefficient is key.\r\nIn the present work, we propose a multiple testing approach to the choice of a suitable threshold using the Bayes false discovery rate (FDR) and a new estimator of the statistical power called average power function (APF) to balance the two types of statistical error. \r\nWe show that the proposed APF behaves well in case of independence of the tests and it is reliable under several dependence conditions. Moreover, we propose a robust method for threshold selection using the 5% and 95% percentiles of APF and FDR bootstrap distributions, respectively, to improve stability.\r\nIn addition, we developed a R-package called APFr which performs APF and Bayes FDR robust estimation and provides simple examples to improve usability. The package has attracted more than 3200 downloads since its publication online (June 2019) at https://CRAN.R-project.org/package=APFr.",
    "email": "N.Margaritella@sms.ed.ac.uk"
  },
  {
    "title": "Flexible Meta-Analysis of Generalized Additive Models with metagam",
    "author": "Øystein Sørensen",
    "affiliation": "Associate Professor, University of Oslo",
    "namesurname": "ØYSTEIN,SØRENSEN",
    "coauthor": "Andreas Brandmaier",
    "track": "R Life Sciences, R Machine Learning & Models",
    "session_type": "Regular talk",
    "description": "Analyzing biomedical data from multiple studies has great potential in terms of increasing statistical power, enabling detection of associations of smaller magnitude than would be possible analyzing each study separately. Restrictions due to privacy or proprietary data as well as more practical concerns can make it hard to share datasets, such that analyzing all data in a single mega-analysis might not be possible. Meta-analytic methods provide a way to overcome this issue, by combining aggregated quantities like model parameters or risk ratios. However, most meta-analytic tools have focused on parametric statistical models, and software for meta-analyzing semi-parametric models like generalized additive models (GAMs) have not been developed. The metagam package attempts to fill this gap: It provides functionality for removing individual participant data from GAM objects such that they can be analyzed in a common location; furthermore metagam enables meta-analysis of the resulting GAM objects, as well as various tools for visualization and statistical analysis. This talk will illustrate use of the metagam package for analysis of the relationship between sleep quality and brain structure using data from six European brain imaging cohorts.",
    "email": "oystein.sorensen.1985@gmail.com"
  },
  {
    "title": "EPIMOD: A computational framework for studying epidemiological systems.",
    "author": "Paolo Castagno",
    "affiliation": "Ph.D.",
    "namesurname": "PAOLO,CASTAGNO",
    "coauthor": "Simone Pernice, Matteo Sereno, Marco Beccuti.",
    "track": "R Life Sciences, R Applications",
    "session_type": "Regular talk",
    "description": "Computational-mathematical models can be efficiently used to provide new insights into drivers of a disease spread, investigating different explanations of observed resurgence and predicting potential effects of different therapies. In this context, we present a new general modeling framework for the analysis of epidemiological and biological systems, characterized by features that make easy its utilization even by researchers without advanced mathematical and computational skills. The implementation of the R package, called “Epimod”, provides a friendly interface to access the model creation and the analysis techniques implemented in the framework. In details, by exploiting the graphical formalism of the Petri Net it is possible to simplify the model creation phase, providing a compact graphical description of the system and an automatically derivation of the underlying stochastic or deterministic process. Then, by using four functions it is possible to deal with Model Generation, Sensitivity Analysis, Model Calibration and Model Analysis phases. Finally, the Docker containerization of all analysis techniques grants a high level of portability and reproducibility. We applied Epimod to model pertussis epidemiology, investigating alternative explanations of its resurgence and to predict potential effects of different vaccination strategies.",
    "email": "castagno@di.unito.it"
  },
  {
    "title": "CorrelAidX - Building R-focused Communities for Social Good on the Local Level",
    "author": "Regina Siegers",
    "affiliation": "CorrelAidX Coordination",
    "namesurname": "REGINA,SIEGERS",
    "coauthor": "Konstantin Gavras",
    "track": "R World",
    "session_type": "Regular talk",
    "description": "Data scientists with their valuable skills have enormous potential to contribute to the social good. This is also true for the R community - and R users seem to be especially motivated to use their skills for the social good, as the overwhelmingly positive reception of Julien Cornebise's keynote \"AI for Good\" at useR2019 (Cornebise 2019) has shown. However, specific strategies for putting the abstract goal \"use data science for the social good\" into practice are often missing, especially in volunteering contexts like the R community, where resources are often limited.\r\n\r\nIn our talk, we present formats that we have implemented on the local level to build R-focused, data-for-good communities across Europe. Originating from the German data4good network CorrelAid with its over 1600 members, we have established 9 local CorrelAidX groups in Germany, the Netherlands, and France.\r\n\r\nThe specific formats build on a three-pillared concept of community building, namely group-bonding, social entrepreneurship and outreach. We present multiple examples that illustrate how our local chapters operate to put data science for good into practice - using the formats of data dialogues, local data4good projects, and CorrelAidX workshops. Lastly, we also outline possibilities to implement such formats in cooperation between CorrelAidX chapters and R community groups such as R user groups or RLadies chapters.",
    "email": "regina.siegers@posteo.de"
  },
  {
    "title": "Interactive visualization of complex texts",
    "author": "Renate Delucchi Danhier",
    "affiliation": "Post-Doc, TU Dortmund",
    "namesurname": "RENATE,DELUCCHI,DANHIER",
    "coauthor": "Paula González Ávalos",
    "track": "R Dataviz & Shiny",
    "session_type": "Regular talk",
    "description": "Hundreds of speakers may describe the same circumstance - e.g. explaining a fixed route to a goal - without producing two identical texts. The enormous variability of language and the complexity involved in encoding meaning poses a real difficulty for linguists analyzing text databases. In order to aid linguists in identifying patterns to perform comparative research, we developed an interactive shiny app that enables quantitative analysis of text corpora without oversimplifying the structure of language. Route directions are an example of complex texts, in which speakers take cognitive decisions such as segmenting the route, selecting landmarks and organizing spatial concepts into sentences. In the data visualization, symbols and colors representing linguistic concepts are plotted into coordinates that relate the information to fixed points along the route. Six interconnected layers of meaning represent the multi-layered form-to-meaning mapping characteristic of language. The shiny app allows to select and deselect information on these different layers, offering a holistic linguistic analysis way beyond the complexity attempted within traditional linguistics. The result is a kind of visual language in itself that deconstructs the interconnected layers of meaning found in natural language.",
    "email": "renatedelucchi@gmail.com"
  },
  {
    "title": "CONNECTOR: a computational approach to study intratumor heterogeneity.",
    "author": "Simone Pernice",
    "affiliation": "Ph.D student at Department of Computer Science of the University of Turin",
    "namesurname": "SIMONE,PERNICE",
    "coauthor": "Beccuti Marco, Sirovich Roberta, Cordero Francesca",
    "track": "R Life Sciences",
    "session_type": "Regular talk",
    "description": "Literature is characterized by a broad class of mathematical models which can be used for fitting cancer growth time series, but with no a global consensus or biological evidence that can drive the choice of the correct model. The conventional perception is that the mechanistic models enable the biological understanding of the systems under study. However, there is no way that these models can capture the variability characterizing the cancer progression, especially because of the irregularity and sparsity of the available data.\r\nFor this reason, we propose CONNECTOR, an R package built on the model-based approach for clustering functional data. Such method is based on the clustering and fitting of the data through a combination of cubic natural splines basis with coefficients treated as random variables. Our approach is particularly effective when the observations are sparse and irregularly spaced, as growth curves usually are. CONNECTOR provides a tool set to easily guide through the parameters selection, i.e., (i) the dimension of the spline basis, (ii) the dimension of the mean space and (iii) the number of clusters to fit, to be properly chosen before fitting. The effectiveness of CONNECTOR is evaluated on growth curves of Patient Derived Xenografts (PDXs) of ovarian cancer. Genomic analyses of PDXs allowed correlating fitted and clustered PDX growth curves to cell population distribution.",
    "email": "simone.pernice@unito.it"
  },
  {
    "title": "gWQS: An R Package for Linear and Generalized Weighted Quantile Sum (WQS) Regression",
    "author": "Stefano Renzetti",
    "affiliation": "PhD Student at Università degli Studi di Milano",
    "namesurname": "STEFANO,RENZETTI",
    "coauthor": "Chris Gennings, Paul C. Curtin",
    "track": "R Machine Learning & Models, R Life Sciences",
    "session_type": "Regular talk",
    "description": "Weighted Quantile Sum (WQS) regression is a statistical model for multivariate regression in high-dimensional datasets commonly encountered in environmental exposures. The model constructs a weighted index estimating the mixture effect associated with all predictor variables on an outcome. The package gWQS extends WQS regression to applications with continuous, categorical and count outcomes. We provide four examples to illustrate the usage of the package.",
    "email": "stefano.renzetti@unibs.it"
  },
  {
    "title": "Transparent Journalism Through the Power of R",
    "author": "Tatjana Kecojevic",
    "affiliation": "SisterAnalyst.org; founder and director",
    "namesurname": "TATJANA,KECOJEVIC",
    "track": "R World",
    "session_type": "Regular talk",
    "description": "This study examines the often-tricky process of delivering data literacy programmes to professionals with most to gain from a deeper understanding of data analysis. As such, the author discusses the process of building and delivering training strategies to journalists in regions where press freedom is constrained by numerous factors, not least of all institutionalised corruption. \r\n\r\nReporting stories that are supplemented with transparent procedural systems are less likely to be contradicted and challenged by vested interest actors. Journalists are able to present findings supported by charts and info graphics, but these are open to translation. Therefore, most importantly, the data and code of the applied analytical methodology should also be available for scrutiny and is less likely to be subverted or prohibited.\r\n\r\nAs part of creating an accessible programme geared to acquiring skills necessary for data journalism, the author takes a step-by-step approach to discussing the actualities of building online platforms for training purposes. Through the use of grammar of graphics in R and Shiny, a web application framework for R, it is possible to develop interactive applications for graphical data visualisation. Presenting findings through interactive and accessible visualisation methods in a transparent and reproducible way is an effective form of reaching audiences that might not otherwise realise the value of the topic or data at hand. \r\n\r\nThe resulting ‘R toolbox for journalists’ is an accessible open-source resource. It can also be adapted to accommodate the need to provide a deeper understanding of the potential for data proficiency to other professions.\r\n\r\nThe accessibility of R allows for users to build support communities, which in the case of journalists is essential for information gathering. Establishing and implementing transparent channels of communication is the key to scrupulous journalism and is why R is so applicable to this objective.",
    "email": "tatjana.keco@gmail.com"
  },
  {
    "title": "Visualising and Modelling Bike Sharing Mobility usage in the city of Milan",
    "author": "Agostino Torti",
    "affiliation": "PhD student - Politecnico di Milano",
    "namesurname": "AGOSTINO,TORTI",
    "coauthor": "Alessia Pini, Simone Vantini",
    "track": "R Dataviz & Shiny",
    "session_type": "Shiny demo",
    "description": "A major trend in modern science is the collection of datasets which are not only “big” but also “complex”. This is particularly true in all sharing mobility systems where data are continuously collected at all time and they are characterised by a high number of features. To extract useful information contained in this huge mass of data, the development of novel statistical techniques and innovative visualization methods are requested.\r\nIn this context, we focused on BikeMi, the main bike sharing system (BSS) in the city of Milan in Italy, and we implemented an R Shiny App to analyse and study people's mobility behaviour across the city. Through the app, it is possible to dynamically select different parameters which allow to visualise the bike sharing flows among the districts of the city according to the hour of the day, the day of the week and the weather conditions. Moreover, a predictive model is implemented in the dashboard allowing to observe the future behaviour or the BSS. By doing this, we would like to both visualize the statistically significant spatio-temporal patterns of the users and to model each possible bike flow in the BikeMi network.",
    "email": "agostino.torti@gmail.com"
  },
  {
    "title": "Media Shiny: Marketing Mix Models Builder",
    "author": "Andrea Melloncelli",
    "affiliation": "VanLog",
    "namesurname": "ANDREA,MELLONCELLI",
    "coauthor": "Mariachiara Fortuna",
    "track": "R Dataviz & Shiny, R Machine Learning & Models, R Production",
    "session_type": "Shiny demo",
    "description": "Marketing Mix Models are used to understand the effects of advertising campaigns. Building such models is challenging: first of all, it requires control of the external effects, such as seasonalities, competitor activities etc. Secondly, it requires to model the decay effect of communication (adstock effect: I do my advertising today, and in two weeks it still has some effect).\r\nThe MediaShiny application allows to build Marketing Mix Models interactively: all steps of MMM, such as selecting, transforming and exploring features (time series), adstock control, model building, evaluation and forecasting can be done interactively. \r\nAs a result the model explains the impact on sales of each media channel (tv, digital, press etc), controlling the external effects. An extra module allows the media budget optimization, using historical data to understand if the level of advertising has no impact or is too high (saturation).\r\n\r\nMediaShiny is a Package App developed with Golem and modularized with Shiny modules. Automatically built and provisioned as a Docker image running in a Shiny Proxy instance. Best User Experience is provided with Drag and Drop and navigation guided by action buttons.",
    "email": "andrea.melloncelli@vanlog.it"
  },
  {
    "title": "ESPRES: A shiny web tool to support River Basin Management planning in European Watersheds",
    "author": "Angel Udias",
    "affiliation": "European Commission, Joint Research Centre (JRC), Ispra, Italy",
    "namesurname": "ANGEL,UDIAS",
    "coauthor": "A. Udias, B. Grizzetti, F. Bouraoui, O Vigiak, A. Pistocchi",
    "track": "R Life Sciences",
    "session_type": "Shiny demo",
    "description": "Integrated river basin management must meet environmental targets while preserving the economic activities of its communities. Stakeholder decisions need to consider conflicting trade-offs between legislative environmental targets and economic activities, while maintaining a basis of transparency and accountability. ESPRES is a shiny web-based Decision Support Tool (DST) that can be used by stakeholders to explore management options in European water bodies. The management options considered in ESPRES are related with the pressures (water use and nutrient application) reduction. \r\nThe shiny web interface provides a point of access to perform analyses of efficient pressure reduction strategies reflecting their perception of costs/effort, political difficulty, and social acceptability of the available solutions. Stakeholders express preferences and perceived difficulties in addressing each environmental pressure by assigning relative weights. The tool include a MOO engine to identified Pareto efficient strategies in terms of maximize the quality in the basin minimizing the total effort for reducing the pressures \r\nAn online version of ESPRES is currently available (www.espres.eu) for four European basins of the Globaqua project, namely the Adige, the Ebro, the Evrotas, and the Sava, to addresses water abstraction and nutrient pollution pressures.",
    "email": "angelluis.udias@gmail.com"
  },
  {
    "title": "tsviz: a data-scientist-friendly addin for RStudio",
    "author": "Emanuele Fabbiani",
    "affiliation": "Chief Data Scientist at xtream, PhD student in Machine Learning",
    "namesurname": "EMANUELE,FABBIANI",
    "coauthor": "Marta Peroni, Riccardo Maganza",
    "track": "R Dataviz & Shiny",
    "session_type": "Shiny demo",
    "description": "In recent years, charting libraries have evolved following two main directions. First, they provided users with as many features as possible and second, they added high-level APIs to easily create the most frequent visualizations. RStudio, with its addins, offers the opportunity to further ease the creation of common plots. \r\n\r\nBorn as an internal project in xtream, tsviz is an open-source Shiny-based addin which contains powerful tools to perform explorative analysis of multivariate time series.\r\n\r\nIts usage is dead simple. Once launched, it scans the global environment for suitable variables. You chose one, and several plots of the time series are shown. Line charts, scatter plots, autocorrelogram, periodogram are only a few examples. Interactivity is achieved by the miniUI framework and the adoption of Plotly charts.\r\n\r\nIts wide adoption among our customers and the overall positive feedback we received demonstrate how addins, usually thought as shortcuts for developers, may provide effective support to data scientists in performing their routine tasks.",
    "email": "donlelef@gmail.com"
  },
  {
    "title": "Mobility scan",
    "author": "Josue Aduna",
    "affiliation": "Behavioural and data scientist at Livemobility",
    "namesurname": "JOSUE,ADUNA",
    "track": "R Dataviz & Shiny",
    "session_type": "Shiny demo",
    "description": "This is a Shiny application designed and developed to foster sustainable mobility behavior under a specific initiative that I currently work in: Livemobility (see https://www.livemobility.com/). \r\n\r\nBroadly speaking, Livemobility is a platform that rewards people for sustainable commuting behavior and helps companies to save money, avoid environmental pollution, improve public health and save travel time. This is achieved through a digital ecosystem that analyses mobility behavior and generates personalized insights to improve mobility efficiency.\r\n\r\nThe Shiny application makes use of web interactive settings together with Google Maps APIs to provide relevant indicators of impact, generate geographic scans and create mobility profiles.",
    "email": "jadunac@gmail.com"
  },
  {
    "title": "Developing Shiny applications to facilitate precision agriculture workflows",
    "author": "Lorenzo Busetto",
    "affiliation": "Institute on Remote Sensing of Enviroment - National Research Council of Italy (CNR-IREA)",
    "namesurname": "LORENZO,BUSETTO",
    "coauthor": "Luigi Ranghetti, Donato Cillis, Maddalena Campi, Saverio Zagaglia, Gabriele Dottori, MIrco Boschetti",
    "track": "R Applications, R Dataviz & Shiny",
    "session_type": "Shiny demo",
    "description": "Precision Agriculture applications rely on geospatial datasets from heterogeneous sources such as crop maps, information about fertilization/phytosanitary treatments, satellite and meteo data, to optimize agricultural practices from an economic and environmental standpoint. Software instruments allowing to easily record, manage and process such datasets are therefore of paramount importance to facilitate, standardize and speed-up the steps required to implement specific workflows. Although required functionalities are available in open source/commercial software, technicians are often required to use different software tools. This affects the time and effort required to replicate a specific workflow on different areas and crop seasons. \r\n\r\nIn this contribution we present our experience in developing two Shiny-based prototypes specifically tailored to the needs of operators of a agro-consulting firm providing precision agriculture services. The first prototype is mainly aimed at providing a simplified, standardized and scalable way to insert, record and query information about agricultural practices, such as crop type/variety, fertilisation and phytosanitary treatments and yield. The second is instead dedicated to facilitating access to satellite imagery data, and applying dedicated processing algorithms for identification of homegenous Management Unit Zones.",
    "email": "lbusett@gmail.com"
  },
  {
    "title": "\"GUInterp\": a Shiny GUI to support spatial interpolation",
    "author": "Luigi Ranghetti",
    "affiliation": "Institute for Remote Sensing of Environment, Consiglio Nazionale delle Ricerche (IREA-CNR)",
    "namesurname": "LUIGI,RANGHETTI",
    "coauthor": "Luigi Ranghetti, Mirco Boschetti, Donato Cillis, Lorenzo Busetto",
    "track": "R Dataviz & Shiny",
    "session_type": "Shiny demo",
    "description": "In this demo we present \"GUInterp\", a Shiny interface written to facilitate the operations required to interpolate point data. A typical spatial interpolation workflow includes common steps: loading point data, filtering them to exclude undesired outlier values, setting the interpolation method and parameters, defining an output raster grid and processing data. Interpolation can be conducted in R using dedicated packages; nevertheless, the availability of an interactive interface could be useful to provide additional control during steps requiring user intervention and to facilitate users with low or no programming skills. \"GUInterp\" was written for this purpose. The user can import input point data, optionally loading a polygon dataset of borders used to constrain the extent of the interpolated outputs. A set of selectors allows filtering input points based on the distribution of the variable to interpolate (which is shown with a reactive histogram) or the spatial position of points (visible on a map). The interpolation can be performed with IDW or Ordinary Kriging methods: in the latter case, the semivariogram can be interactively defined and optimised using a dedicated interface. Further settings can be exploited to tune computation requirements (RAM usage, amount of time) on the basis of available hardware or user needs. \"GUInterp\" is released as R package under the GNU GPL-3 license.",
    "email": "ranghetti.l@irea.cnr.it"
  },
  {
    "title": "A demonstration of ABACUS: Apps Based Activities for Communicating and Understanding Statistics",
    "author": "Mintu Nath",
    "affiliation": "Medical Statistics Team, Institute of Applied Health Sciences, University of Aberdeen, AB25 2ZD, UK",
    "namesurname": "MINTU,NATH",
    "track": "R Dataviz & Shiny",
    "session_type": "Shiny demo",
    "description": "ABACUS, developed using Shiny framework, is a set of applications for effective communication and understanding in statistics. It is currently available as an R package. Users who are not familiar with R programming can also access the applications through its web-based interface. The current version of ABACUS includes properties of Normal distribution, properties of the sampling distribution, one-sample z and t tests, two samples unpaired t-test and analysis of variance and comparison of Normal and t distributions. Using an example, the shiny demonstration will include the essential features of the application particularly its relevance in generating data across wide-ranging disciplines, its interactive elements and identifying best practices for presentation of results and interpretation of statistical outputs.",
    "email": "dr.m.nath@gmail.com"
  },
  {
    "title": "Scoring the Implicit Association Test has never been easier: DscoreApp",
    "author": "Ottavia M. Epifania",
    "affiliation": "University of Padova (IT)",
    "namesurname": "OTTAVIA,M.,EPIFANIA",
    "coauthor": "Anselmi Pasquale, Robusto Egidio",
    "track": "R Dataviz & Shiny",
    "session_type": "Shiny demo",
    "description": "Throughout the past decades, the interest in the implicit investigation of attitudes and preferences has been constantly growing among social scientists, and the Implicit Association Test (IAT) is one of the most common measures used for this aim. The so-called “IAT effect” (i.e., the difference in respondents’ performance between two contrasting categorization tasks) is usually expressed by the D-score. Despite that several options exist for computing the D-score, including R packages and SPSS syntaxes, none of them provides either an easy to use interface or a means for immediately visualizing the results. A Shiny Web application (DscoreApp) was developed to provide IAT users with an easy to use and powerful tool for the computation of the D-score. DscoreApp allows users to upload their IAT data, decide which specific D-score algorithm to compute, and immediately see the results in easy to read and interactive graphs. At the end of the computation, users can download a data frame containing the computed D-score and other information on respondents’ performance, such as the proportion of correct responses or the number of trials exceeding a time threshold. Graphical representations can be downloaded as well. Besides providing an easy to use and open source tool for computing the D-score, DscoreApp allows for grasping an immediate overview of the results, and to visually inspect them.",
    "email": "otta.epifania@gmail.com"
  },
  {
    "title": "rTRhexNG: Hexagon sticker app for rTRNG",
    "author": "Riccardo Porreca",
    "affiliation": "R Enthusiast at Mirai Solutions",
    "namesurname": "RICCARDO,PORRECA",
    "track": "R Dataviz & Shiny",
    "session_type": "Shiny demo",
    "description": "Hexagon stickers have become a popular way to make software tools, and R packages in particular, visually recognizable and stand out as landmarks in an ever-growing ecosystem. In general, good hexagon logos are not only visually appealing but also convey the key aspects of a package with their graphical design.\r\nIn this talk, we will showcase rTRhexNG (https://github.com/miraisolutions/rTRhexNG#readme), a Shiny app built for creating the hexagon sticker of the rTRNG (https://github.com/miraisolutions/rTRNG#readme) package. The core idea behind the logo was to have an appealing design that would at the same time illustrate the key features of the package: jump and split operations on (pseudo-)random sequences. Leveraging on the simple yet powerful SVG image format, R was used to automate the creation and location of several visual elements representing random sequences, and a Shiny app was built on top to quickly assess different designs in an interactive way.\r\nWe demonstrate the Shiny app in action to concretely explain what jump and split mean in rTRNG, and show how the sticker design naturally emerges from their visual representation. The power of this interactive yet automated approach was invaluable to fine-tune the final look of the sticker, also allowing to easily explore alternative polygon or circle designs the implementation naturally extends to.",
    "email": "riccardo.porreca@mirai-solutions.com"
  }
]
